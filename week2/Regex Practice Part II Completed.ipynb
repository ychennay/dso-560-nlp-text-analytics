{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#Import-Dependencies-and-Load-in-Dataset\" data-toc-modified-id=\"Import-Dependencies-and-Load-in-Dataset-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>Import Dependencies and Load in Dataset</a></span></li><li><span><a href=\"#Pandas-Exploratory-Data-Analysis\" data-toc-modified-id=\"Pandas-Exploratory-Data-Analysis-2\"><span class=\"toc-item-num\">2&nbsp;&nbsp;</span>Pandas Exploratory Data Analysis</a></span><ul class=\"toc-item\"><li><ul class=\"toc-item\"><li><span><a href=\"#Show-the-First/Last-Rows-of-Dataset\" data-toc-modified-id=\"Show-the-First/Last-Rows-of-Dataset-2.0.1\"><span class=\"toc-item-num\">2.0.1&nbsp;&nbsp;</span>Show the First/Last Rows of Dataset</a></span></li><li><span><a href=\"#Drop-Unnecessary-Pandas-Column\" data-toc-modified-id=\"Drop-Unnecessary-Pandas-Column-2.0.2\"><span class=\"toc-item-num\">2.0.2&nbsp;&nbsp;</span>Drop Unnecessary Pandas Column</a></span></li><li><span><a href=\"#Compute-a-New-Pandas-Series-and-Attach-to-Dataframe-as-Column\" data-toc-modified-id=\"Compute-a-New-Pandas-Series-and-Attach-to-Dataframe-as-Column-2.0.3\"><span class=\"toc-item-num\">2.0.3&nbsp;&nbsp;</span>Compute a New Pandas Series and Attach to Dataframe as Column</a></span></li><li><span><a href=\"#Count-Number-of-Rows-in-Dataframe-Meet-Some-Criteria\" data-toc-modified-id=\"Count-Number-of-Rows-in-Dataframe-Meet-Some-Criteria-2.0.4\"><span class=\"toc-item-num\">2.0.4&nbsp;&nbsp;</span>Count Number of Rows in Dataframe Meet Some Criteria</a></span></li><li><span><a href=\"#Filter-DataFrame-for-a-Subset-of-Rows\" data-toc-modified-id=\"Filter-DataFrame-for-a-Subset-of-Rows-2.0.5\"><span class=\"toc-item-num\">2.0.5&nbsp;&nbsp;</span>Filter DataFrame for a Subset of Rows</a></span></li></ul></li><li><span><a href=\"#Difference-Between-extractall-and-findall\" data-toc-modified-id=\"Difference-Between-extractall-and-findall-2.1\"><span class=\"toc-item-num\">2.1&nbsp;&nbsp;</span>Difference Between <code>extractall</code> and <code>findall</code></a></span></li></ul></li><li><span><a href=\"#Regex-Character-Classes\" data-toc-modified-id=\"Regex-Character-Classes-3\"><span class=\"toc-item-num\">3&nbsp;&nbsp;</span>Regex Character Classes</a></span><ul class=\"toc-item\"><li><ul class=\"toc-item\"><li><span><a href=\"#Find-all-Tweets-That-Start-with-a-Number\" data-toc-modified-id=\"Find-all-Tweets-That-Start-with-a-Number-3.0.1\"><span class=\"toc-item-num\">3.0.1&nbsp;&nbsp;</span>Find all Tweets That Start with a Number</a></span></li><li><span><a href=\"#Find-all-@-Mentions\" data-toc-modified-id=\"Find-all-@-Mentions-3.0.2\"><span class=\"toc-item-num\">3.0.2&nbsp;&nbsp;</span>Find all @ Mentions</a></span><ul class=\"toc-item\"><li><span><a href=\"#Alternative-Method-to-Work-With-Lists-in-Pandas\" data-toc-modified-id=\"Alternative-Method-to-Work-With-Lists-in-Pandas-3.0.2.1\"><span class=\"toc-item-num\">3.0.2.1&nbsp;&nbsp;</span>Alternative Method to Work With Lists in Pandas</a></span></li></ul></li></ul></li></ul></li><li><span><a href=\"#Quantifiers\" data-toc-modified-id=\"Quantifiers-4\"><span class=\"toc-item-num\">4&nbsp;&nbsp;</span>Quantifiers</a></span><ul class=\"toc-item\"><li><ul class=\"toc-item\"><li><span><a href=\"#Match-All-Phone-Numbers-(Link)\" data-toc-modified-id=\"Match-All-Phone-Numbers-(Link)-4.0.1\"><span class=\"toc-item-num\">4.0.1&nbsp;&nbsp;</span>Match All Phone Numbers (<a href=\"https://regexr.com/50v17\" target=\"_blank\">Link</a>)</a></span></li><li><span><a href=\"#Parse-Out-Zip-Codes-(Link)\" data-toc-modified-id=\"Parse-Out-Zip-Codes-(Link)-4.0.2\"><span class=\"toc-item-num\">4.0.2&nbsp;&nbsp;</span>Parse Out Zip Codes (<a href=\"https://regexr.com/50v1g\" target=\"_blank\">Link</a>)</a></span></li></ul></li></ul></li><li><span><a href=\"#Capture-Groups\" data-toc-modified-id=\"Capture-Groups-5\"><span class=\"toc-item-num\">5&nbsp;&nbsp;</span>Capture Groups</a></span><ul class=\"toc-item\"><li><ul class=\"toc-item\"><li><span><a href=\"#Example:-Parsing-out-Weekday-from-Timestamp-String-(Link)\" data-toc-modified-id=\"Example:-Parsing-out-Weekday-from-Timestamp-String-(Link)-5.0.1\"><span class=\"toc-item-num\">5.0.1&nbsp;&nbsp;</span>Example: Parsing out Weekday from Timestamp String (<a href=\"https://regexr.com/50v0l\" target=\"_blank\">Link</a>)</a></span></li><li><span><a href=\"#Example:-Parsing-Out-Domain-Names\" data-toc-modified-id=\"Example:-Parsing-Out-Domain-Names-5.0.2\"><span class=\"toc-item-num\">5.0.2&nbsp;&nbsp;</span>Example: Parsing Out Domain Names</a></span></li></ul></li><li><span><a href=\"#Non-Capture-Groups\" data-toc-modified-id=\"Non-Capture-Groups-5.1\"><span class=\"toc-item-num\">5.1&nbsp;&nbsp;</span>Non-Capture Groups</a></span><ul class=\"toc-item\"><li><span><a href=\"#Example-1-(Link)\" data-toc-modified-id=\"Example-1-(Link)-5.1.1\"><span class=\"toc-item-num\">5.1.1&nbsp;&nbsp;</span>Example 1 <a href=\"https://regexr.com/50t7c\" target=\"_blank\">(Link)</a></a></span></li><li><span><a href=\"#Example-2-(Link)\" data-toc-modified-id=\"Example-2-(Link)-5.1.2\"><span class=\"toc-item-num\">5.1.2&nbsp;&nbsp;</span>Example 2 <a href=\"https://regexr.com/50t7c\" target=\"_blank\">(Link)</a></a></span></li><li><span><a href=\"#Example-3-(Link)\" data-toc-modified-id=\"Example-3-(Link)-5.1.3\"><span class=\"toc-item-num\">5.1.3&nbsp;&nbsp;</span>Example 3 (<a href=\"https://regexr.com/50ush\" target=\"_blank\">Link</a>)</a></span></li></ul></li></ul></li><li><span><a href=\"#Exercises\" data-toc-modified-id=\"Exercises-6\"><span class=\"toc-item-num\">6&nbsp;&nbsp;</span>Exercises</a></span><ul class=\"toc-item\"><li><ul class=\"toc-item\"><li><ul class=\"toc-item\"><li><span><a href=\"#Find-the-subject-headings-for-these-emails.\" data-toc-modified-id=\"Find-the-subject-headings-for-these-emails.-6.0.0.1\"><span class=\"toc-item-num\">6.0.0.1&nbsp;&nbsp;</span>Find the subject headings for these emails.</a></span></li><li><span><a href=\"#Find-all-hashtags-mentioned-in-the-tweets_df-dataset.-Store-it-as-a-separate-column-called-hashtags.\" data-toc-modified-id=\"Find-all-hashtags-mentioned-in-the-tweets_df-dataset.-Store-it-as-a-separate-column-called-hashtags.-6.0.0.2\"><span class=\"toc-item-num\">6.0.0.2&nbsp;&nbsp;</span>Find all hashtags mentioned in the <code>tweets_df</code> dataset. Store it as a separate column called <strong>hashtags</strong>.</a></span></li><li><span><a href=\"#In-fraudulent_emails.txt,-identify-the-list-of-email-addresses-for-your-security-administrator-to-blacklist-from-your-company's-email-servers.\" data-toc-modified-id=\"In-fraudulent_emails.txt,-identify-the-list-of-email-addresses-for-your-security-administrator-to-blacklist-from-your-company's-email-servers.-6.0.0.3\"><span class=\"toc-item-num\">6.0.0.3&nbsp;&nbsp;</span>In <code>fraudulent_emails.txt</code>, identify the <strong>list of email addresses</strong> for your security administrator to blacklist from your company's email servers.</a></span></li><li><span><a href=\"#In-fraudulent_emails.txt,-identify-any-IP-addresses-that-should-be-blacklisted\" data-toc-modified-id=\"In-fraudulent_emails.txt,-identify-any-IP-addresses-that-should-be-blacklisted-6.0.0.4\"><span class=\"toc-item-num\">6.0.0.4&nbsp;&nbsp;</span>In <code>fraudulent_emails.txt</code>, identify any IP addresses that should be blacklisted</a></span></li><li><span><a href=\"#The-word-&quot;AT&amp;T&quot;-is-not-spelled-correctly-in-the-tweets_df-dataset.-Correct-the-misspelling.\" data-toc-modified-id=\"The-word-&quot;AT&amp;T&quot;-is-not-spelled-correctly-in-the-tweets_df-dataset.-Correct-the-misspelling.-6.0.0.5\"><span class=\"toc-item-num\">6.0.0.5&nbsp;&nbsp;</span>The word \"AT&amp;T\" is not spelled correctly in the <code>tweets_df</code> dataset. Correct the misspelling.</a></span></li><li><span><a href=\"#Removing-hashtags-and-mentions,-which-topic-has-the-highest-average-tweet-character-count?\" data-toc-modified-id=\"Removing-hashtags-and-mentions,-which-topic-has-the-highest-average-tweet-character-count?-6.0.0.6\"><span class=\"toc-item-num\">6.0.0.6&nbsp;&nbsp;</span>Removing hashtags and mentions, which topic has the highest average tweet character count?</a></span></li></ul></li></ul></li></ul></li></ul></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import Dependencies and Load in Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "tweets_df: pd.DataFrame = pd.read_csv(\"../datasets/tweets_pandas.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pandas Exploratory Data Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Show the First/Last Rows of Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>likes</th>\n",
       "      <th>index</th>\n",
       "      <th>date</th>\n",
       "      <th>topic</th>\n",
       "      <th>handle</th>\n",
       "      <th>tweet</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>495</th>\n",
       "      <td>4</td>\n",
       "      <td>14074</td>\n",
       "      <td>Sun Jun 14 04:36:34 UTC 2009</td>\n",
       "      <td>latex</td>\n",
       "      <td>iamtheonlyjosie</td>\n",
       "      <td>Ahhh... back in a *real* text editing environm...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>496</th>\n",
       "      <td>0</td>\n",
       "      <td>14075</td>\n",
       "      <td>Sun Jun 14 21:36:07 UTC 2009</td>\n",
       "      <td>iran</td>\n",
       "      <td>plutopup7</td>\n",
       "      <td>Trouble in Iran, I see. Hmm. Iran. Iran so far...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>497</th>\n",
       "      <td>0</td>\n",
       "      <td>14076</td>\n",
       "      <td>Sun Jun 14 21:36:17 UTC 2009</td>\n",
       "      <td>iran</td>\n",
       "      <td>captain_pete</td>\n",
       "      <td>Reading the tweets coming out of Iran... The w...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     likes  index                          date  topic           handle  \\\n",
       "495      4  14074  Sun Jun 14 04:36:34 UTC 2009  latex  iamtheonlyjosie   \n",
       "496      0  14075  Sun Jun 14 21:36:07 UTC 2009   iran        plutopup7   \n",
       "497      0  14076  Sun Jun 14 21:36:17 UTC 2009   iran     captain_pete   \n",
       "\n",
       "                                                 tweet  \n",
       "495  Ahhh... back in a *real* text editing environm...  \n",
       "496  Trouble in Iran, I see. Hmm. Iran. Iran so far...  \n",
       "497  Reading the tweets coming out of Iran... The w...  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweets_df.tail(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Drop Unnecessary Pandas Column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets_df.drop(columns=[\"index\"], inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compute a New Pandas Series and Attach to Dataframe as Column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>likes</th>\n",
       "      <th>date</th>\n",
       "      <th>topic</th>\n",
       "      <th>handle</th>\n",
       "      <th>tweet</th>\n",
       "      <th>tweet_length</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>4</td>\n",
       "      <td>Mon May 11 03:17:40 UTC 2009</td>\n",
       "      <td>kindle2</td>\n",
       "      <td>tpryan</td>\n",
       "      <td>@stellargirl I loooooooovvvvvveee my Kindle2. ...</td>\n",
       "      <td>111</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4</td>\n",
       "      <td>Mon May 11 03:18:03 UTC 2009</td>\n",
       "      <td>kindle2</td>\n",
       "      <td>vcu451</td>\n",
       "      <td>Reading my kindle2...  Love it... Lee childs i...</td>\n",
       "      <td>58</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4</td>\n",
       "      <td>Mon May 11 03:18:54 UTC 2009</td>\n",
       "      <td>kindle2</td>\n",
       "      <td>chadfu</td>\n",
       "      <td>Ok, first assesment of the #kindle2 ...it fuck...</td>\n",
       "      <td>58</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>Mon May 11 03:19:04 UTC 2009</td>\n",
       "      <td>kindle2</td>\n",
       "      <td>SIX15</td>\n",
       "      <td>@kenburbary You'll love your Kindle2. I've had...</td>\n",
       "      <td>140</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>Mon May 11 03:21:41 UTC 2009</td>\n",
       "      <td>kindle2</td>\n",
       "      <td>yamarama</td>\n",
       "      <td>@mikefish  Fair enough. But i have the Kindle2...</td>\n",
       "      <td>75</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>493</th>\n",
       "      <td>2</td>\n",
       "      <td>Sun Jun 14 04:31:43 UTC 2009</td>\n",
       "      <td>latex</td>\n",
       "      <td>proggit</td>\n",
       "      <td>Ask Programming: LaTeX or InDesign?: submitted...</td>\n",
       "      <td>102</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>494</th>\n",
       "      <td>0</td>\n",
       "      <td>Sun Jun 14 04:32:17 UTC 2009</td>\n",
       "      <td>latex</td>\n",
       "      <td>sam33r</td>\n",
       "      <td>On that note, I hate Word. I hate Pages. I hat...</td>\n",
       "      <td>125</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>495</th>\n",
       "      <td>4</td>\n",
       "      <td>Sun Jun 14 04:36:34 UTC 2009</td>\n",
       "      <td>latex</td>\n",
       "      <td>iamtheonlyjosie</td>\n",
       "      <td>Ahhh... back in a *real* text editing environm...</td>\n",
       "      <td>65</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>496</th>\n",
       "      <td>0</td>\n",
       "      <td>Sun Jun 14 21:36:07 UTC 2009</td>\n",
       "      <td>iran</td>\n",
       "      <td>plutopup7</td>\n",
       "      <td>Trouble in Iran, I see. Hmm. Iran. Iran so far...</td>\n",
       "      <td>94</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>497</th>\n",
       "      <td>0</td>\n",
       "      <td>Sun Jun 14 21:36:17 UTC 2009</td>\n",
       "      <td>iran</td>\n",
       "      <td>captain_pete</td>\n",
       "      <td>Reading the tweets coming out of Iran... The w...</td>\n",
       "      <td>92</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>498 rows Ã— 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     likes                          date    topic           handle  \\\n",
       "0        4  Mon May 11 03:17:40 UTC 2009  kindle2           tpryan   \n",
       "1        4  Mon May 11 03:18:03 UTC 2009  kindle2           vcu451   \n",
       "2        4  Mon May 11 03:18:54 UTC 2009  kindle2           chadfu   \n",
       "3        4  Mon May 11 03:19:04 UTC 2009  kindle2            SIX15   \n",
       "4        4  Mon May 11 03:21:41 UTC 2009  kindle2         yamarama   \n",
       "..     ...                           ...      ...              ...   \n",
       "493      2  Sun Jun 14 04:31:43 UTC 2009    latex          proggit   \n",
       "494      0  Sun Jun 14 04:32:17 UTC 2009    latex           sam33r   \n",
       "495      4  Sun Jun 14 04:36:34 UTC 2009    latex  iamtheonlyjosie   \n",
       "496      0  Sun Jun 14 21:36:07 UTC 2009     iran        plutopup7   \n",
       "497      0  Sun Jun 14 21:36:17 UTC 2009     iran     captain_pete   \n",
       "\n",
       "                                                 tweet  tweet_length  \n",
       "0    @stellargirl I loooooooovvvvvveee my Kindle2. ...           111  \n",
       "1    Reading my kindle2...  Love it... Lee childs i...            58  \n",
       "2    Ok, first assesment of the #kindle2 ...it fuck...            58  \n",
       "3    @kenburbary You'll love your Kindle2. I've had...           140  \n",
       "4    @mikefish  Fair enough. But i have the Kindle2...            75  \n",
       "..                                                 ...           ...  \n",
       "493  Ask Programming: LaTeX or InDesign?: submitted...           102  \n",
       "494  On that note, I hate Word. I hate Pages. I hat...           125  \n",
       "495  Ahhh... back in a *real* text editing environm...            65  \n",
       "496  Trouble in Iran, I see. Hmm. Iran. Iran so far...            94  \n",
       "497  Reading the tweets coming out of Iran... The w...            92  \n",
       "\n",
       "[498 rows x 6 columns]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get length of tweets in characters\n",
    "tweets_df[\"tweet_length\"] =tweets_df[\"tweet\"].apply(len)\n",
    "tweets_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Count Number of Rows in Dataframe Meet Some Criteria"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# count number of times Obama appears in tweets\n",
    "tweets_df[\"contains_obama\"] = tweets_df[\"tweet\"].str.contains(r'\\bObama\\b')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Filter DataFrame for a Subset of Rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>likes</th>\n",
       "      <th>date</th>\n",
       "      <th>topic</th>\n",
       "      <th>handle</th>\n",
       "      <th>tweet</th>\n",
       "      <th>tweet_length</th>\n",
       "      <th>contains_obama</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>4</td>\n",
       "      <td>Mon May 11 03:29:20 UTC 2009</td>\n",
       "      <td>obama</td>\n",
       "      <td>mandanicole</td>\n",
       "      <td>how can you not love Obama? he makes jokes abo...</td>\n",
       "      <td>57</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>2</td>\n",
       "      <td>Mon May 11 03:32:42 UTC 2009</td>\n",
       "      <td>obama</td>\n",
       "      <td>jpeb</td>\n",
       "      <td>Check this video out -- President Obama at the...</td>\n",
       "      <td>101</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>0</td>\n",
       "      <td>Mon May 11 03:32:48 UTC 2009</td>\n",
       "      <td>obama</td>\n",
       "      <td>kylesellers</td>\n",
       "      <td>@Karoli I firmly believe that Obama/Pelosi hav...</td>\n",
       "      <td>140</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>4</td>\n",
       "      <td>Mon May 11 03:33:38 UTC 2009</td>\n",
       "      <td>obama</td>\n",
       "      <td>theviewfans</td>\n",
       "      <td>House Correspondents dinner was last night who...</td>\n",
       "      <td>106</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>0</td>\n",
       "      <td>Sun May 17 17:32:00 UTC 2009</td>\n",
       "      <td>aig</td>\n",
       "      <td>KennyTRoland</td>\n",
       "      <td>?Obama Administration Must Stop Bonuses to AIG...</td>\n",
       "      <td>85</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>102</th>\n",
       "      <td>0</td>\n",
       "      <td>Mon May 25 17:35:23 UTC 2009</td>\n",
       "      <td>cheney</td>\n",
       "      <td>jepaco</td>\n",
       "      <td>Dick Cheney's dishonest speech about torture, ...</td>\n",
       "      <td>102</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>243</th>\n",
       "      <td>4</td>\n",
       "      <td>Mon May 11 03:29:01 UTC 2009</td>\n",
       "      <td>obama</td>\n",
       "      <td>lingbellbell</td>\n",
       "      <td>Obama is quite a good comedian! check out his ...</td>\n",
       "      <td>87</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>244</th>\n",
       "      <td>4</td>\n",
       "      <td>Mon May 11 03:29:38 UTC 2009</td>\n",
       "      <td>obama</td>\n",
       "      <td>motsandco</td>\n",
       "      <td>' Barack Obama shows his funny side \" &amp;gt;&amp;gt;...</td>\n",
       "      <td>82</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>245</th>\n",
       "      <td>4</td>\n",
       "      <td>Mon May 11 03:29:41 UTC 2009</td>\n",
       "      <td>obama</td>\n",
       "      <td>mickou</td>\n",
       "      <td>I like this guy : ' Barack Obama shows his fun...</td>\n",
       "      <td>85</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>246</th>\n",
       "      <td>4</td>\n",
       "      <td>Mon May 11 03:32:35 UTC 2009</td>\n",
       "      <td>obama</td>\n",
       "      <td>meggentile</td>\n",
       "      <td>Obama's speech was pretty awesome last night! ...</td>\n",
       "      <td>65</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>247</th>\n",
       "      <td>4</td>\n",
       "      <td>Mon May 11 03:33:05 UTC 2009</td>\n",
       "      <td>obama</td>\n",
       "      <td>failness</td>\n",
       "      <td>Reading  \"Bill Clinton Fail - Obama Win?\" http...</td>\n",
       "      <td>67</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>248</th>\n",
       "      <td>4</td>\n",
       "      <td>Mon May 11 03:33:43 UTC 2009</td>\n",
       "      <td>obama</td>\n",
       "      <td>kledy</td>\n",
       "      <td>Obama More Popular Than U.S. Among Arabs: Surv...</td>\n",
       "      <td>140</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>249</th>\n",
       "      <td>4</td>\n",
       "      <td>Mon May 11 03:34:04 UTC 2009</td>\n",
       "      <td>obama</td>\n",
       "      <td>LaurelEdelstein</td>\n",
       "      <td>Obama's got JOKES!! haha just got to watch a b...</td>\n",
       "      <td>128</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>330</th>\n",
       "      <td>0</td>\n",
       "      <td>Mon May 25 17:19:30 UTC 2009</td>\n",
       "      <td>north korea</td>\n",
       "      <td>Mvsic</td>\n",
       "      <td>Listening to Obama... Friggin North Korea...</td>\n",
       "      <td>44</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>331</th>\n",
       "      <td>0</td>\n",
       "      <td>Mon May 25 17:21:16 UTC 2009</td>\n",
       "      <td>pelosi</td>\n",
       "      <td>CFURNAROS</td>\n",
       "      <td>I just realized we three monkeys in the white ...</td>\n",
       "      <td>83</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>375</th>\n",
       "      <td>2</td>\n",
       "      <td>Tue Jun 02 03:00:51 UTC 2009</td>\n",
       "      <td>gm</td>\n",
       "      <td>economywire</td>\n",
       "      <td>Obama: Nationalization of GM to be short-term ...</td>\n",
       "      <td>78</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     likes                          date        topic           handle  \\\n",
       "9        4  Mon May 11 03:29:20 UTC 2009        obama      mandanicole   \n",
       "10       2  Mon May 11 03:32:42 UTC 2009        obama             jpeb   \n",
       "11       0  Mon May 11 03:32:48 UTC 2009        obama      kylesellers   \n",
       "12       4  Mon May 11 03:33:38 UTC 2009        obama      theviewfans   \n",
       "48       0  Sun May 17 17:32:00 UTC 2009          aig     KennyTRoland   \n",
       "102      0  Mon May 25 17:35:23 UTC 2009       cheney           jepaco   \n",
       "243      4  Mon May 11 03:29:01 UTC 2009        obama     lingbellbell   \n",
       "244      4  Mon May 11 03:29:38 UTC 2009        obama        motsandco   \n",
       "245      4  Mon May 11 03:29:41 UTC 2009        obama           mickou   \n",
       "246      4  Mon May 11 03:32:35 UTC 2009        obama       meggentile   \n",
       "247      4  Mon May 11 03:33:05 UTC 2009        obama         failness   \n",
       "248      4  Mon May 11 03:33:43 UTC 2009        obama            kledy   \n",
       "249      4  Mon May 11 03:34:04 UTC 2009        obama  LaurelEdelstein   \n",
       "330      0  Mon May 25 17:19:30 UTC 2009  north korea            Mvsic   \n",
       "331      0  Mon May 25 17:21:16 UTC 2009       pelosi        CFURNAROS   \n",
       "375      2  Tue Jun 02 03:00:51 UTC 2009           gm      economywire   \n",
       "\n",
       "                                                 tweet  tweet_length  \\\n",
       "9    how can you not love Obama? he makes jokes abo...            57   \n",
       "10   Check this video out -- President Obama at the...           101   \n",
       "11   @Karoli I firmly believe that Obama/Pelosi hav...           140   \n",
       "12   House Correspondents dinner was last night who...           106   \n",
       "48   ?Obama Administration Must Stop Bonuses to AIG...            85   \n",
       "102  Dick Cheney's dishonest speech about torture, ...           102   \n",
       "243  Obama is quite a good comedian! check out his ...            87   \n",
       "244  ' Barack Obama shows his funny side \" &gt;&gt;...            82   \n",
       "245  I like this guy : ' Barack Obama shows his fun...            85   \n",
       "246  Obama's speech was pretty awesome last night! ...            65   \n",
       "247  Reading  \"Bill Clinton Fail - Obama Win?\" http...            67   \n",
       "248  Obama More Popular Than U.S. Among Arabs: Surv...           140   \n",
       "249  Obama's got JOKES!! haha just got to watch a b...           128   \n",
       "330       Listening to Obama... Friggin North Korea...            44   \n",
       "331  I just realized we three monkeys in the white ...            83   \n",
       "375  Obama: Nationalization of GM to be short-term ...            78   \n",
       "\n",
       "     contains_obama  \n",
       "9              True  \n",
       "10             True  \n",
       "11             True  \n",
       "12             True  \n",
       "48             True  \n",
       "102            True  \n",
       "243            True  \n",
       "244            True  \n",
       "245            True  \n",
       "246            True  \n",
       "247            True  \n",
       "248            True  \n",
       "249            True  \n",
       "330            True  \n",
       "331            True  \n",
       "375            True  "
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweets_df[tweets_df[\"contains_obama\"]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Difference Between `extractall` and `findall`\n",
    "\n",
    "The `extractall` method will return a fanned-out multi-dimensional index. For example, in the example below, the primary index is `0`, but there are sub-indices for `stellargirl` (`0`), `loooooooovvvvvveee` (`1`), etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th>match</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th rowspan=\"5\" valign=\"top\">0</th>\n",
       "      <th>0</th>\n",
       "      <td>stellargirl</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>loooooooovvvvvveee</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Kindle2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Not</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>that</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                          0\n",
       "  match                    \n",
       "0 0             stellargirl\n",
       "  1      loooooooovvvvvveee\n",
       "  2                 Kindle2\n",
       "  3                     Not\n",
       "  4                    that"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweets_df[\"tweet\"].str.extractall(r'\\b(\\w{3,})\\b').head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `findall` method will return a list of results (or an empty list if there is no match)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "618"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweets_df[\"tweet\"].str.findall(r'\\b(\\w{8,})\\b').apply(len).sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Regex Character Classes\n",
    "\n",
    "There are often shortcut keywords you can use instead of typing out every possible character you want to match against. For instance, instead of `[a-zA-Z0-9]`, for all practical purposes, you can type out `/w`.\n",
    "\n",
    "![Character Classes](https://raw.githubusercontent.com/ychennay/dso-560-nlp-text-analytics/main/images/character_chars.png \"Regex character classes\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Find all Tweets That Start with a Number"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* [Non-Match Example](https://regexr.com/50ur7)\n",
    "* [Match Example](https://regexr.com/50urj)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['child', 'children']"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = \"I have a child and want multiple children\"\n",
    "\n",
    "re.findall(r'\\bchild(?:ren)?\\b', text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Find all @ Mentions\n",
    "We'll use the `\\w` character class to match for mentions (ie. `@ychennay`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0       True\n",
       "1      False\n",
       "2      False\n",
       "3      False\n",
       "4      False\n",
       "       ...  \n",
       "493    False\n",
       "494    False\n",
       "495    False\n",
       "496    False\n",
       "497    False\n",
       "Name: tweet, Length: 498, dtype: bool"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweets_df[\"tweet\"].str.findall(r'@\\w+').apply(lambda x: \"@stellargirl\" in x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Quantifiers\n",
    "\n",
    "Quantifiers let you specify how many times a character or group should be matched.\n",
    "![Quantifiers](https://raw.githubusercontent.com/ychennay/dso-560-nlp-text-analytics/main/images/quantifiers.png \"Regex character classes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['aaaaaaaaaaa', 'aaaaaaa']"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = \"aaaaaaaaaaabbbbbbaaaaaaa\"\n",
    "import re\n",
    "\n",
    "re.findall(r'a{3,}', text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"my mom and my dad\"\n",
    "\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['michael', 'christy']"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "email_address = \"yuchen@gmail9com michael@yahoo.com christy@usc.edu\"\n",
    "\n",
    "re.findall(r'(\\w+)@\\w+\\.(?:com|edu)', email_address)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['mom', 'dad']"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "re.findall(r'(mom|dad)', text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "solution2": "shown",
    "solution2_first": true
   },
   "source": [
    "### Match All Phone Numbers ([Link](https://regexr.com/50v17))\n",
    "In the unwanted callers dataset (`unwanted_calls.csv`), parse out all phone numbers.\n",
    "\n",
    "We'll only consider for the time being phone numbers that follow the format `123-456-7890`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "solution2": "shown"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "solution2": "shown",
    "solution2_first": true
   },
   "source": [
    "### Parse Out Zip Codes ([Link](https://regexr.com/50v1g))\n",
    "In the `location_center_point_of_the_zip_code` field, we store both zip codes as well as geolocation data (latitudes and longitudes). We'll only consider zip codes with 5 digits (not the +4 digit delivery route)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "solution2": "shown"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Capture Groups\n",
    "\n",
    "[Oracle documentation on Capture Groups:](https://docs.oracle.com/javase/tutorial/essential/regex/groups.html)\n",
    "> Capturing groups are a way to treat **multiple characters as a single unit**. They are created by placing the characters to be grouped inside a set of parentheses. For example, the regular expression `(dog)` creates a single group containing the letters `d`, `o`, and `g`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example: Parsing out Weekday from Timestamp String ([Link](https://regexr.com/50v0l))\n",
    "\n",
    "In the `date` field of `tweets_df`, we have timestamp strings that look like this: `Mon May 11 03:22:30 UTC 2009`. We want to parse out the weekday from this string."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example: Parsing Out Domain Names\n",
    "We want to capture the domain names of different websites. Here, we need to escape the `.` part of `www.google.com`. In regex, `.` means \"anything\". To actually indicate we want to match for the literal `.` period character, we need to escape it, using `\\.`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "websites = open(\"list_of_websites.txt\").readlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Non-Capture Groups"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sometimes you want to group multiple characters into a single unit to apply regex operations on them, but you don't\n",
    "want to actually capture or return their result."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example 1 [(Link)](https://regexr.com/50t7c)\n",
    "Using non-capture groups to match for optional text:\n",
    "\n",
    "You want to capture both `child` and `children` in your text:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['', 'ren']"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = \"The word child is singular, but the word children is plural. \"\n",
    "\n",
    "re.findall(r'\\bchild(ren)?\\b', text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example 2 [(Link)](https://regexr.com/50t7c)\n",
    "Find all the mentions in the tweets. Mentions start with `@`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example 3 ([Link](https://regexr.com/50ush))\n",
    "Here we want to match for the dollar amount, but we don't want to include the currency notation (`$` or `USD`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"Average fast food wage is $9.08, but inflation has increased USD9.23\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "solution2": "shown"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercises\n",
    "\n",
    "You'll be using the the `tweets_df` Pandas dataframe and `fraudulent_emails.txt` text files for these examples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "email_text = open(\"../datasets/fraudulent_emails.txt\").read()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Find the subject headings for these emails.\n",
    "**Hint**: Look for the subject line within the email."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Find all hashtags mentioned in the `tweets_df` dataset. Store it as a separate column called **hashtags**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### In `fraudulent_emails.txt`, identify the **list of email addresses** for your security administrator to blacklist from your company's email servers.\n",
    "\n",
    "* Not all emails are malicious! Provide only the list of email addresses from where the email originates from. **Hint**: identify the pattern in the emails that tells you the source of the email.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### In `fraudulent_emails.txt`, identify any IP addresses that should be blacklisted\n",
    "\n",
    "An IPv4 address goes from **1.1.1.1 to 255.255.255.255**. For now, just worry about the number of digits, not whether the value in the address is above `255`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The word \"AT&T\" is not spelled correctly in the `tweets_df` dataset. Correct the misspelling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Removing hashtags and mentions, which topic has the highest average tweet character count?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "solution2": "hidden"
   },
   "outputs": [],
   "source": [
    "tweets_df = tweets_df[\"tweet\"].str.replace(r'AT&amp;', 'AT&')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "solution2": "hidden"
   },
   "outputs": [],
   "source": [
    "# first, we'll find the rows that contain AT&T to see what the pattern looks like\n",
    "tweets_df[tweets_df[\"tweet\"].str.contains(r'(AT)')].head()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "225px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
