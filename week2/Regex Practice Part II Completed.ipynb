{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#Import-Dependencies-and-Load-in-Dataset\" data-toc-modified-id=\"Import-Dependencies-and-Load-in-Dataset-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>Import Dependencies and Load in Dataset</a></span></li><li><span><a href=\"#Pandas-Exploratory-Data-Analysis\" data-toc-modified-id=\"Pandas-Exploratory-Data-Analysis-2\"><span class=\"toc-item-num\">2&nbsp;&nbsp;</span>Pandas Exploratory Data Analysis</a></span><ul class=\"toc-item\"><li><ul class=\"toc-item\"><li><span><a href=\"#Show-the-First/Last-Rows-of-Dataset\" data-toc-modified-id=\"Show-the-First/Last-Rows-of-Dataset-2.0.1\"><span class=\"toc-item-num\">2.0.1&nbsp;&nbsp;</span>Show the First/Last Rows of Dataset</a></span></li><li><span><a href=\"#Drop-Unnecessary-Pandas-Column\" data-toc-modified-id=\"Drop-Unnecessary-Pandas-Column-2.0.2\"><span class=\"toc-item-num\">2.0.2&nbsp;&nbsp;</span>Drop Unnecessary Pandas Column</a></span></li><li><span><a href=\"#Compute-a-New-Pandas-Series-and-Attach-to-Dataframe-as-Column\" data-toc-modified-id=\"Compute-a-New-Pandas-Series-and-Attach-to-Dataframe-as-Column-2.0.3\"><span class=\"toc-item-num\">2.0.3&nbsp;&nbsp;</span>Compute a New Pandas Series and Attach to Dataframe as Column</a></span></li><li><span><a href=\"#Count-Number-of-Rows-in-Dataframe-Meet-Some-Criteria\" data-toc-modified-id=\"Count-Number-of-Rows-in-Dataframe-Meet-Some-Criteria-2.0.4\"><span class=\"toc-item-num\">2.0.4&nbsp;&nbsp;</span>Count Number of Rows in Dataframe Meet Some Criteria</a></span></li><li><span><a href=\"#Filter-DataFrame-for-a-Subset-of-Rows\" data-toc-modified-id=\"Filter-DataFrame-for-a-Subset-of-Rows-2.0.5\"><span class=\"toc-item-num\">2.0.5&nbsp;&nbsp;</span>Filter DataFrame for a Subset of Rows</a></span></li></ul></li><li><span><a href=\"#Difference-Between-extractall-and-findall\" data-toc-modified-id=\"Difference-Between-extractall-and-findall-2.1\"><span class=\"toc-item-num\">2.1&nbsp;&nbsp;</span>Difference Between <code>extractall</code> and <code>findall</code></a></span></li></ul></li><li><span><a href=\"#Regex-Character-Classes\" data-toc-modified-id=\"Regex-Character-Classes-3\"><span class=\"toc-item-num\">3&nbsp;&nbsp;</span>Regex Character Classes</a></span><ul class=\"toc-item\"><li><ul class=\"toc-item\"><li><span><a href=\"#Find-all-Tweets-That-Start-with-a-Number\" data-toc-modified-id=\"Find-all-Tweets-That-Start-with-a-Number-3.0.1\"><span class=\"toc-item-num\">3.0.1&nbsp;&nbsp;</span>Find all Tweets That Start with a Number</a></span></li><li><span><a href=\"#Find-all-@-Mentions\" data-toc-modified-id=\"Find-all-@-Mentions-3.0.2\"><span class=\"toc-item-num\">3.0.2&nbsp;&nbsp;</span>Find all @ Mentions</a></span><ul class=\"toc-item\"><li><span><a href=\"#Alternative-Method-to-Work-With-Lists-in-Pandas\" data-toc-modified-id=\"Alternative-Method-to-Work-With-Lists-in-Pandas-3.0.2.1\"><span class=\"toc-item-num\">3.0.2.1&nbsp;&nbsp;</span>Alternative Method to Work With Lists in Pandas</a></span></li></ul></li></ul></li></ul></li><li><span><a href=\"#Quantifiers\" data-toc-modified-id=\"Quantifiers-4\"><span class=\"toc-item-num\">4&nbsp;&nbsp;</span>Quantifiers</a></span><ul class=\"toc-item\"><li><ul class=\"toc-item\"><li><span><a href=\"#Match-All-Phone-Numbers-(Link)\" data-toc-modified-id=\"Match-All-Phone-Numbers-(Link)-4.0.1\"><span class=\"toc-item-num\">4.0.1&nbsp;&nbsp;</span>Match All Phone Numbers (<a href=\"https://regexr.com/50v17\" target=\"_blank\">Link</a>)</a></span></li><li><span><a href=\"#Parse-Out-Zip-Codes-(Link)\" data-toc-modified-id=\"Parse-Out-Zip-Codes-(Link)-4.0.2\"><span class=\"toc-item-num\">4.0.2&nbsp;&nbsp;</span>Parse Out Zip Codes (<a href=\"https://regexr.com/50v1g\" target=\"_blank\">Link</a>)</a></span></li></ul></li></ul></li><li><span><a href=\"#Capture-Groups\" data-toc-modified-id=\"Capture-Groups-5\"><span class=\"toc-item-num\">5&nbsp;&nbsp;</span>Capture Groups</a></span><ul class=\"toc-item\"><li><ul class=\"toc-item\"><li><span><a href=\"#Example:-Parsing-out-Weekday-from-Timestamp-String-(Link)\" data-toc-modified-id=\"Example:-Parsing-out-Weekday-from-Timestamp-String-(Link)-5.0.1\"><span class=\"toc-item-num\">5.0.1&nbsp;&nbsp;</span>Example: Parsing out Weekday from Timestamp String (<a href=\"https://regexr.com/50v0l\" target=\"_blank\">Link</a>)</a></span></li><li><span><a href=\"#Example:-Parsing-Out-Domain-Names\" data-toc-modified-id=\"Example:-Parsing-Out-Domain-Names-5.0.2\"><span class=\"toc-item-num\">5.0.2&nbsp;&nbsp;</span>Example: Parsing Out Domain Names</a></span></li></ul></li><li><span><a href=\"#Non-Capture-Groups\" data-toc-modified-id=\"Non-Capture-Groups-5.1\"><span class=\"toc-item-num\">5.1&nbsp;&nbsp;</span>Non-Capture Groups</a></span><ul class=\"toc-item\"><li><span><a href=\"#Example-1-(Link)\" data-toc-modified-id=\"Example-1-(Link)-5.1.1\"><span class=\"toc-item-num\">5.1.1&nbsp;&nbsp;</span>Example 1 <a href=\"https://regexr.com/50t7c\" target=\"_blank\">(Link)</a></a></span></li><li><span><a href=\"#Example-2-(Link)\" data-toc-modified-id=\"Example-2-(Link)-5.1.2\"><span class=\"toc-item-num\">5.1.2&nbsp;&nbsp;</span>Example 2 <a href=\"https://regexr.com/50t7c\" target=\"_blank\">(Link)</a></a></span></li><li><span><a href=\"#Example-3-(Link)\" data-toc-modified-id=\"Example-3-(Link)-5.1.3\"><span class=\"toc-item-num\">5.1.3&nbsp;&nbsp;</span>Example 3 (<a href=\"https://regexr.com/50ush\" target=\"_blank\">Link</a>)</a></span></li></ul></li></ul></li><li><span><a href=\"#Exercises\" data-toc-modified-id=\"Exercises-6\"><span class=\"toc-item-num\">6&nbsp;&nbsp;</span>Exercises</a></span><ul class=\"toc-item\"><li><ul class=\"toc-item\"><li><ul class=\"toc-item\"><li><span><a href=\"#Find-the-subject-headings-for-these-emails.\" data-toc-modified-id=\"Find-the-subject-headings-for-these-emails.-6.0.0.1\"><span class=\"toc-item-num\">6.0.0.1&nbsp;&nbsp;</span>Find the subject headings for these emails.</a></span></li><li><span><a href=\"#Find-all-hashtags-mentioned-in-the-tweets_df-dataset.-Store-it-as-a-separate-column-called-hashtags.\" data-toc-modified-id=\"Find-all-hashtags-mentioned-in-the-tweets_df-dataset.-Store-it-as-a-separate-column-called-hashtags.-6.0.0.2\"><span class=\"toc-item-num\">6.0.0.2&nbsp;&nbsp;</span>Find all hashtags mentioned in the <code>tweets_df</code> dataset. Store it as a separate column called <strong>hashtags</strong>.</a></span></li><li><span><a href=\"#In-fraudulent_emails.txt,-identify-the-list-of-email-addresses-for-your-security-administrator-to-blacklist-from-your-company's-email-servers.\" data-toc-modified-id=\"In-fraudulent_emails.txt,-identify-the-list-of-email-addresses-for-your-security-administrator-to-blacklist-from-your-company's-email-servers.-6.0.0.3\"><span class=\"toc-item-num\">6.0.0.3&nbsp;&nbsp;</span>In <code>fraudulent_emails.txt</code>, identify the <strong>list of email addresses</strong> for your security administrator to blacklist from your company's email servers.</a></span></li><li><span><a href=\"#In-fraudulent_emails.txt,-identify-any-IP-addresses-that-should-be-blacklisted\" data-toc-modified-id=\"In-fraudulent_emails.txt,-identify-any-IP-addresses-that-should-be-blacklisted-6.0.0.4\"><span class=\"toc-item-num\">6.0.0.4&nbsp;&nbsp;</span>In <code>fraudulent_emails.txt</code>, identify any IP addresses that should be blacklisted</a></span></li><li><span><a href=\"#The-word-&quot;AT&amp;T&quot;-is-not-spelled-correctly-in-the-tweets_df-dataset.-Correct-the-misspelling.\" data-toc-modified-id=\"The-word-&quot;AT&amp;T&quot;-is-not-spelled-correctly-in-the-tweets_df-dataset.-Correct-the-misspelling.-6.0.0.5\"><span class=\"toc-item-num\">6.0.0.5&nbsp;&nbsp;</span>The word \"AT&amp;T\" is not spelled correctly in the <code>tweets_df</code> dataset. Correct the misspelling.</a></span></li><li><span><a href=\"#Removing-hashtags-and-mentions,-which-topic-has-the-highest-average-tweet-character-count?\" data-toc-modified-id=\"Removing-hashtags-and-mentions,-which-topic-has-the-highest-average-tweet-character-count?-6.0.0.6\"><span class=\"toc-item-num\">6.0.0.6&nbsp;&nbsp;</span>Removing hashtags and mentions, which topic has the highest average tweet character count?</a></span></li></ul></li></ul></li></ul></li></ul></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import Dependencies and Load in Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "tweets_df: pd.DataFrame = pd.read_csv(\"tweets_pandas.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pandas Exploratory Data Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Show the First/Last Rows of Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets_df.tail(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Drop Unnecessary Pandas Column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets_df.drop(columns=[\"index\"], inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compute a New Pandas Series and Attach to Dataframe as Column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>likes</th>\n",
       "      <th>date</th>\n",
       "      <th>topic</th>\n",
       "      <th>handle</th>\n",
       "      <th>tweet</th>\n",
       "      <th>tweet_length</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>4</td>\n",
       "      <td>Mon May 11 03:17:40 UTC 2009</td>\n",
       "      <td>kindle2</td>\n",
       "      <td>tpryan</td>\n",
       "      <td>@stellargirl I loooooooovvvvvveee my Kindle2. ...</td>\n",
       "      <td>111</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4</td>\n",
       "      <td>Mon May 11 03:18:03 UTC 2009</td>\n",
       "      <td>kindle2</td>\n",
       "      <td>vcu451</td>\n",
       "      <td>Reading my kindle2...  Love it... Lee childs i...</td>\n",
       "      <td>58</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4</td>\n",
       "      <td>Mon May 11 03:18:54 UTC 2009</td>\n",
       "      <td>kindle2</td>\n",
       "      <td>chadfu</td>\n",
       "      <td>Ok, first assesment of the #kindle2 ...it fuck...</td>\n",
       "      <td>58</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>Mon May 11 03:19:04 UTC 2009</td>\n",
       "      <td>kindle2</td>\n",
       "      <td>SIX15</td>\n",
       "      <td>@kenburbary You'll love your Kindle2. I've had...</td>\n",
       "      <td>140</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>Mon May 11 03:21:41 UTC 2009</td>\n",
       "      <td>kindle2</td>\n",
       "      <td>yamarama</td>\n",
       "      <td>@mikefish  Fair enough. But i have the Kindle2...</td>\n",
       "      <td>75</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>493</th>\n",
       "      <td>2</td>\n",
       "      <td>Sun Jun 14 04:31:43 UTC 2009</td>\n",
       "      <td>latex</td>\n",
       "      <td>proggit</td>\n",
       "      <td>Ask Programming: LaTeX or InDesign?: submitted...</td>\n",
       "      <td>102</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>494</th>\n",
       "      <td>0</td>\n",
       "      <td>Sun Jun 14 04:32:17 UTC 2009</td>\n",
       "      <td>latex</td>\n",
       "      <td>sam33r</td>\n",
       "      <td>On that note, I hate Word. I hate Pages. I hat...</td>\n",
       "      <td>125</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>495</th>\n",
       "      <td>4</td>\n",
       "      <td>Sun Jun 14 04:36:34 UTC 2009</td>\n",
       "      <td>latex</td>\n",
       "      <td>iamtheonlyjosie</td>\n",
       "      <td>Ahhh... back in a *real* text editing environm...</td>\n",
       "      <td>65</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>496</th>\n",
       "      <td>0</td>\n",
       "      <td>Sun Jun 14 21:36:07 UTC 2009</td>\n",
       "      <td>iran</td>\n",
       "      <td>plutopup7</td>\n",
       "      <td>Trouble in Iran, I see. Hmm. Iran. Iran so far...</td>\n",
       "      <td>94</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>497</th>\n",
       "      <td>0</td>\n",
       "      <td>Sun Jun 14 21:36:17 UTC 2009</td>\n",
       "      <td>iran</td>\n",
       "      <td>captain_pete</td>\n",
       "      <td>Reading the tweets coming out of Iran... The w...</td>\n",
       "      <td>92</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>498 rows Ã— 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     likes                          date    topic           handle  \\\n",
       "0        4  Mon May 11 03:17:40 UTC 2009  kindle2           tpryan   \n",
       "1        4  Mon May 11 03:18:03 UTC 2009  kindle2           vcu451   \n",
       "2        4  Mon May 11 03:18:54 UTC 2009  kindle2           chadfu   \n",
       "3        4  Mon May 11 03:19:04 UTC 2009  kindle2            SIX15   \n",
       "4        4  Mon May 11 03:21:41 UTC 2009  kindle2         yamarama   \n",
       "..     ...                           ...      ...              ...   \n",
       "493      2  Sun Jun 14 04:31:43 UTC 2009    latex          proggit   \n",
       "494      0  Sun Jun 14 04:32:17 UTC 2009    latex           sam33r   \n",
       "495      4  Sun Jun 14 04:36:34 UTC 2009    latex  iamtheonlyjosie   \n",
       "496      0  Sun Jun 14 21:36:07 UTC 2009     iran        plutopup7   \n",
       "497      0  Sun Jun 14 21:36:17 UTC 2009     iran     captain_pete   \n",
       "\n",
       "                                                 tweet  tweet_length  \n",
       "0    @stellargirl I loooooooovvvvvveee my Kindle2. ...           111  \n",
       "1    Reading my kindle2...  Love it... Lee childs i...            58  \n",
       "2    Ok, first assesment of the #kindle2 ...it fuck...            58  \n",
       "3    @kenburbary You'll love your Kindle2. I've had...           140  \n",
       "4    @mikefish  Fair enough. But i have the Kindle2...            75  \n",
       "..                                                 ...           ...  \n",
       "493  Ask Programming: LaTeX or InDesign?: submitted...           102  \n",
       "494  On that note, I hate Word. I hate Pages. I hat...           125  \n",
       "495  Ahhh... back in a *real* text editing environm...            65  \n",
       "496  Trouble in Iran, I see. Hmm. Iran. Iran so far...            94  \n",
       "497  Reading the tweets coming out of Iran... The w...            92  \n",
       "\n",
       "[498 rows x 6 columns]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get length of tweets in characters\n",
    "tweets_df[\"tweet_length\"] =tweets_df[\"tweet\"].apply(len)\n",
    "tweets_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Count Number of Rows in Dataframe Meet Some Criteria"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Obamacare"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# count number of times Obama appears in tweets\n",
    "tweets_df[\"contains_obama\"] = tweets_df[\"tweet\"].str.contains(r'\\bObama\\b')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Filter DataFrame for a Subset of Rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>likes</th>\n",
       "      <th>date</th>\n",
       "      <th>topic</th>\n",
       "      <th>handle</th>\n",
       "      <th>tweet</th>\n",
       "      <th>tweet_length</th>\n",
       "      <th>contains_obama</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>4</td>\n",
       "      <td>Mon May 11 03:29:20 UTC 2009</td>\n",
       "      <td>obama</td>\n",
       "      <td>mandanicole</td>\n",
       "      <td>how can you not love Obama? he makes jokes abo...</td>\n",
       "      <td>57</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>2</td>\n",
       "      <td>Mon May 11 03:32:42 UTC 2009</td>\n",
       "      <td>obama</td>\n",
       "      <td>jpeb</td>\n",
       "      <td>Check this video out -- President Obama at the...</td>\n",
       "      <td>101</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>0</td>\n",
       "      <td>Mon May 11 03:32:48 UTC 2009</td>\n",
       "      <td>obama</td>\n",
       "      <td>kylesellers</td>\n",
       "      <td>@Karoli I firmly believe that Obama/Pelosi hav...</td>\n",
       "      <td>140</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>4</td>\n",
       "      <td>Mon May 11 03:33:38 UTC 2009</td>\n",
       "      <td>obama</td>\n",
       "      <td>theviewfans</td>\n",
       "      <td>House Correspondents dinner was last night who...</td>\n",
       "      <td>106</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>0</td>\n",
       "      <td>Sun May 17 17:32:00 UTC 2009</td>\n",
       "      <td>aig</td>\n",
       "      <td>KennyTRoland</td>\n",
       "      <td>?Obama Administration Must Stop Bonuses to AIG...</td>\n",
       "      <td>85</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>102</th>\n",
       "      <td>0</td>\n",
       "      <td>Mon May 25 17:35:23 UTC 2009</td>\n",
       "      <td>cheney</td>\n",
       "      <td>jepaco</td>\n",
       "      <td>Dick Cheney's dishonest speech about torture, ...</td>\n",
       "      <td>102</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>243</th>\n",
       "      <td>4</td>\n",
       "      <td>Mon May 11 03:29:01 UTC 2009</td>\n",
       "      <td>obama</td>\n",
       "      <td>lingbellbell</td>\n",
       "      <td>Obama is quite a good comedian! check out his ...</td>\n",
       "      <td>87</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>244</th>\n",
       "      <td>4</td>\n",
       "      <td>Mon May 11 03:29:38 UTC 2009</td>\n",
       "      <td>obama</td>\n",
       "      <td>motsandco</td>\n",
       "      <td>' Barack Obama shows his funny side \" &amp;gt;&amp;gt;...</td>\n",
       "      <td>82</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>245</th>\n",
       "      <td>4</td>\n",
       "      <td>Mon May 11 03:29:41 UTC 2009</td>\n",
       "      <td>obama</td>\n",
       "      <td>mickou</td>\n",
       "      <td>I like this guy : ' Barack Obama shows his fun...</td>\n",
       "      <td>85</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>246</th>\n",
       "      <td>4</td>\n",
       "      <td>Mon May 11 03:32:35 UTC 2009</td>\n",
       "      <td>obama</td>\n",
       "      <td>meggentile</td>\n",
       "      <td>Obama's speech was pretty awesome last night! ...</td>\n",
       "      <td>65</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>247</th>\n",
       "      <td>4</td>\n",
       "      <td>Mon May 11 03:33:05 UTC 2009</td>\n",
       "      <td>obama</td>\n",
       "      <td>failness</td>\n",
       "      <td>Reading  \"Bill Clinton Fail - Obama Win?\" http...</td>\n",
       "      <td>67</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>248</th>\n",
       "      <td>4</td>\n",
       "      <td>Mon May 11 03:33:43 UTC 2009</td>\n",
       "      <td>obama</td>\n",
       "      <td>kledy</td>\n",
       "      <td>Obama More Popular Than U.S. Among Arabs: Surv...</td>\n",
       "      <td>140</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>249</th>\n",
       "      <td>4</td>\n",
       "      <td>Mon May 11 03:34:04 UTC 2009</td>\n",
       "      <td>obama</td>\n",
       "      <td>LaurelEdelstein</td>\n",
       "      <td>Obama's got JOKES!! haha just got to watch a b...</td>\n",
       "      <td>128</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>330</th>\n",
       "      <td>0</td>\n",
       "      <td>Mon May 25 17:19:30 UTC 2009</td>\n",
       "      <td>north korea</td>\n",
       "      <td>Mvsic</td>\n",
       "      <td>Listening to Obama... Friggin North Korea...</td>\n",
       "      <td>44</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>331</th>\n",
       "      <td>0</td>\n",
       "      <td>Mon May 25 17:21:16 UTC 2009</td>\n",
       "      <td>pelosi</td>\n",
       "      <td>CFURNAROS</td>\n",
       "      <td>I just realized we three monkeys in the white ...</td>\n",
       "      <td>83</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>375</th>\n",
       "      <td>2</td>\n",
       "      <td>Tue Jun 02 03:00:51 UTC 2009</td>\n",
       "      <td>gm</td>\n",
       "      <td>economywire</td>\n",
       "      <td>Obama: Nationalization of GM to be short-term ...</td>\n",
       "      <td>78</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     likes                          date        topic           handle  \\\n",
       "9        4  Mon May 11 03:29:20 UTC 2009        obama      mandanicole   \n",
       "10       2  Mon May 11 03:32:42 UTC 2009        obama             jpeb   \n",
       "11       0  Mon May 11 03:32:48 UTC 2009        obama      kylesellers   \n",
       "12       4  Mon May 11 03:33:38 UTC 2009        obama      theviewfans   \n",
       "48       0  Sun May 17 17:32:00 UTC 2009          aig     KennyTRoland   \n",
       "102      0  Mon May 25 17:35:23 UTC 2009       cheney           jepaco   \n",
       "243      4  Mon May 11 03:29:01 UTC 2009        obama     lingbellbell   \n",
       "244      4  Mon May 11 03:29:38 UTC 2009        obama        motsandco   \n",
       "245      4  Mon May 11 03:29:41 UTC 2009        obama           mickou   \n",
       "246      4  Mon May 11 03:32:35 UTC 2009        obama       meggentile   \n",
       "247      4  Mon May 11 03:33:05 UTC 2009        obama         failness   \n",
       "248      4  Mon May 11 03:33:43 UTC 2009        obama            kledy   \n",
       "249      4  Mon May 11 03:34:04 UTC 2009        obama  LaurelEdelstein   \n",
       "330      0  Mon May 25 17:19:30 UTC 2009  north korea            Mvsic   \n",
       "331      0  Mon May 25 17:21:16 UTC 2009       pelosi        CFURNAROS   \n",
       "375      2  Tue Jun 02 03:00:51 UTC 2009           gm      economywire   \n",
       "\n",
       "                                                 tweet  tweet_length  \\\n",
       "9    how can you not love Obama? he makes jokes abo...            57   \n",
       "10   Check this video out -- President Obama at the...           101   \n",
       "11   @Karoli I firmly believe that Obama/Pelosi hav...           140   \n",
       "12   House Correspondents dinner was last night who...           106   \n",
       "48   ?Obama Administration Must Stop Bonuses to AIG...            85   \n",
       "102  Dick Cheney's dishonest speech about torture, ...           102   \n",
       "243  Obama is quite a good comedian! check out his ...            87   \n",
       "244  ' Barack Obama shows his funny side \" &gt;&gt;...            82   \n",
       "245  I like this guy : ' Barack Obama shows his fun...            85   \n",
       "246  Obama's speech was pretty awesome last night! ...            65   \n",
       "247  Reading  \"Bill Clinton Fail - Obama Win?\" http...            67   \n",
       "248  Obama More Popular Than U.S. Among Arabs: Surv...           140   \n",
       "249  Obama's got JOKES!! haha just got to watch a b...           128   \n",
       "330       Listening to Obama... Friggin North Korea...            44   \n",
       "331  I just realized we three monkeys in the white ...            83   \n",
       "375  Obama: Nationalization of GM to be short-term ...            78   \n",
       "\n",
       "     contains_obama  \n",
       "9              True  \n",
       "10             True  \n",
       "11             True  \n",
       "12             True  \n",
       "48             True  \n",
       "102            True  \n",
       "243            True  \n",
       "244            True  \n",
       "245            True  \n",
       "246            True  \n",
       "247            True  \n",
       "248            True  \n",
       "249            True  \n",
       "330            True  \n",
       "331            True  \n",
       "375            True  "
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweets_df[tweets_df[\"contains_obama\"]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Difference Between `extractall` and `findall`\n",
    "\n",
    "The `extractall` method will return a fanned-out multi-dimensional index. For example, in the example below, the primary index is `0`, but there are sub-indices for `stellargirl` (`0`), `loooooooovvvvvveee` (`1`), etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th>match</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th rowspan=\"5\" valign=\"top\">0</th>\n",
       "      <th>0</th>\n",
       "      <td>stellargirl</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>loooooooovvvvvveee</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Kindle2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Not</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>that</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                          0\n",
       "  match                    \n",
       "0 0             stellargirl\n",
       "  1      loooooooovvvvvveee\n",
       "  2                 Kindle2\n",
       "  3                     Not\n",
       "  4                    that"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweets_df[\"tweet\"].str.extractall(r'\\b(\\w{3,})\\b').head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `findall` method will return a list of results (or an empty list if there is no match)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "618"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweets_df[\"tweet\"].str.findall(r'\\b(\\w{8,})\\b').apply(len).sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Regex Character Classes\n",
    "\n",
    "There are often shortcut keywords you can use instead of typing out every possible character you want to match against. For instance, instead of `[a-zA-Z0-9]`, for all practical purposes, you can type out `/w`.\n",
    "\n",
    "![Character Classes](images/character_chars.png \"Regex character classes\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Find all Tweets That Start with a Number"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* [Non-Match Example](https://regexr.com/50ur7)\n",
    "* [Match Example](https://regexr.com/50urj)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['child', 'children']"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = \"I have a child and want multiple children\"\n",
    "\n",
    "re.findall(r'\\bchild(?:ren)?\\b', text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"I like dogs. I like cats.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['@stellargirl I loooooooovvvvvveee my Kindle2. Not that the DX is cool, but the 2 is fantastic in its own right.',\n",
       " 'Reading my kindle2...  Love it... Lee childs is good read.',\n",
       " 'Ok, first assesment of the #kindle2 ...it fucking rocks!!!',\n",
       " \"@kenburbary You'll love your Kindle2. I've had mine for a few months and never looked back. The new big one is huge! No need for remorse! :)\",\n",
       " \"@mikefish  Fair enough. But i have the Kindle2 and I think it's perfect  :)\",\n",
       " \"@richardebaker no. it is too big. I'm quite happy with the Kindle2.\",\n",
       " \"#lebron best athlete of our generation, if not all time (basketball related) I don't want to get into inter-sport debates about   __1/2\",\n",
       " \"@ludajuice Lebron is a Beast, but I'm still cheering 4 the A..til the end.\",\n",
       " '[#MLUC09] Customer Innovation Award Winner: Booz Allen Hamilton -- http://ping.fm/c2hPP',\n",
       " '@SoChi2 I current use the Nikon D90 and love it, but not as much as the Canon 40D/50D. I chose the D90 for the  video feature. My mistake.',\n",
       " \"@phyreman9 Google is always a good place to look. Should've mentioned I worked on the Mustang w/ my Dad, @KimbleT.\",\n",
       " 'I\\'m listening to \"P.Y.T\" by Danny Gokey &lt;3 &lt;3 &lt;3 Aww, he\\'s so amazing. I &lt;3 him so much :)',\n",
       " \"glad i didnt do Bay to Breakers today, it's 1000 freaking degrees in San Francisco wtf\",\n",
       " 'Bill Simmons in conversation with Malcolm Gladwell http://bit.ly/j9o50',\n",
       " \"@morind45 Because the twitter api is slow and most client's aren't good.\",\n",
       " \"By the way, I'm totally inspired by this freaky Nike commercial: http://snurl.com/icgj9\",\n",
       " 'Brand New Canon EOS 50D 15MP DSLR Camera Canon 17-85mm IS Lens ...: Web Technology Thread, Brand New Canon EOS 5.. http://u.mavrev.com/5a3t',\n",
       " '@accannis @edog1203 Great Stanford course. Thanks for making it available to the public! Really helpful and informative for starting off!',\n",
       " \"NVIDIA Names Stanford's Bill Dally Chief Scientist, VP Of Research http://bit.ly/Fvvg9\",\n",
       " 'Cheney and Bush are the real culprits - http://fwix.com/article/939496',\n",
       " 'Life?s a bitch? and so is Dick Cheney. #p2 #bipart #tlot #tcot #hhrs #GOP #DNC http://is.gd/DjyQ',\n",
       " 'jQuery UI 1.6 Book Review - http://cfbloggers.org/?c=30631',\n",
       " 'Very Interesting Ad from Adobe by Goodby, Silverstein &amp; Partners - YouTube - Adobe CS4: Le Sens Propre http://bit.ly/VprpT',\n",
       " 'The ever amazing Psyop and Goodby Silverstein &amp; Partners for HP! http://bit.ly/g2rU8 Have to go play with After Effects now!',\n",
       " 'top ten most watched on Viral-Video Chart.  Love the nike #mostvaluablepuppets campaign from Wieden &amp; Kennedy http://bit.ly/nR1n9',\n",
       " 'zomg!!! I have a G2!!!!!!!',\n",
       " 'Ok so lots of buzz from IO2009 but how lucky are they - a Free G2!! http://is.gd/Hyzl',\n",
       " 'just got a free G2 android at google i/o!!!',\n",
       " \"Guess I'll be retiring my G1 and start using my developer G2 woot #googleio\",\n",
       " 'Hi there, does anyone have a great source for advice on viral marketing?... http://link.gs/YtZ8',\n",
       " \"Here's A case study on how to use viral marketing to add over 10,000 people to your list http://snipr.com/i50oz\",\n",
       " \"VIRAL MARKETING FAIL. This Acia Pills brand oughta get shut down for hacking into people's messenger's.  i get 5-6 msgs in a day! Arrrgh!\",\n",
       " 'Just saw the new Night at the Museum movie...it was...okay...lol 7\\\\10',\n",
       " 'Going to see night at the museum 2 with tall boy',\n",
       " '@shannyoday I will take you on a date to see night at the museum 2 whenever you want...it looks soooooo good',\n",
       " 'saw night at the museum 2 last night.. pretty crazy movie.. but the cast was awesome so it was well worth it. Robin Williams forever!',\n",
       " 'Night at the Museum tonite instead of UP. :( oh well. that 4 yr old better enjoy it. LOL',\n",
       " 'F*ck Time Warner Cable!!! You f*cking suck balls!!! I have a $700 HD tv &amp; my damn HD channels hardly ever come in. Bullshit!!',\n",
       " 'Rocawear Heads to China, Building 300 Stores  - http://tinyurl.com/nofet3',\n",
       " 'Three China aerospace giants develop Tianjin Binhai  New Area,  22.9 B yuan invested   http://bit.ly/mMiDv',\n",
       " 'just started playing Major League Baseball 2K9. http://raptr.com/H3LLGWAR',\n",
       " 'Sony coupon code.. Expires soon.. http://www.coupondork.com/r/1796',\n",
       " '@XPhile1908 I have three words for you: \"Safeway dot com\"',\n",
       " 'iPhone May Get Radio Tagging and Nike  : Recently-released iTunes version 8.2 suggests that VoiceOver functional.. http://tinyurl.com/oq5ctc',\n",
       " 'Beginning JavaScript and CSS Development with jQuery #javascript #css #jquery http://bit.ly/TO3e5',\n",
       " 'RT @blknprecious1: RT GREAT @dbroos \"Someone\\'s sitting in the shade today because someone planted a tree a long time ago.\"- Warren Buffet',\n",
       " 'Warren Buffet became (for a time) the richest man in the United States, not by working but investing in 1 Big idea which lead to the fortune',\n",
       " 'According to the create a school, Notre Dame will have 7 receivers in NCAA 10 at 84 or higher rating :) *sweet*',\n",
       " \"@BlondeBroad it's definitely under warranty &amp; my experience is the amazon support for kindle is great! had to contact them about my kindle2\",\n",
       " 'RT Look, Available !Amazon Kindle2 &amp; Kindle DX, Get it Here: http://short.to/87ub The Top Electronic Book Reader Period, free 2 day ship ...',\n",
       " 'Time Warner cable phone reps r dumber than nails!!!!! UGH! Cable was working 10 mins ago now its not WTF!',\n",
       " \"pissed about at&amp;t's mid-contract upgrade price for the iPhone (it's $200 more) I'm not going to pay $499 for something I thought was $299\",\n",
       " 'Safari 4 is fast :) Even on my shitty AT&amp;T tethering.',\n",
       " \"@springsingfiend @dvyers @sethdaggett @jlshack AT&amp;T dropped the ball and isn't supporting crap with the new iPhone 3.0... FAIL #att SUCKS!!!\",\n",
       " 'Talk is Cheap: Bing that, I?ll stick with Google. http://bit.ly/XC3C8',\n",
       " 'reading on my new Kindle2!',\n",
       " 'My Kindle2 came and I LOVE it! :)',\n",
       " 'LOVING my new Kindle2.  Named her Kendra in case u were wondering. The \"cookbook\" is THE tool cuz it tells u all the tricks!  Best gift EVR!',\n",
       " '45 Pros You Should Be Following on Twitter - http://is.gd/sMbZ',\n",
       " 'Reading  \"Bill Clinton Fail - Obama Win?\" http://tinyurl.com/pcyxj7',\n",
       " 'LEbron james got in a car accident i guess..just heard it on evening news...wow i cant believe it..will he be ok ? http://twtad.com/69750',\n",
       " \"@uscsports21 LeBron is a monsta and he is only 24. SMH The world ain't ready.\",\n",
       " \"@asherroth World Cup 2010 Access?? Damn, that's a good look!\",\n",
       " 'Just bought my tickets for the 2010 FIFA World Cup in South Africa. Its going to be a great summer. http://bit.ly/9GEZI',\n",
       " 'The great Indian tamasha truly will unfold from May 16, the result day for Indian General Election.',\n",
       " \"@crlane I have the Kindle2. I've seen pictures of the DX, but haven't seen it in person. I love my Kindle - I'm on it everyday.\",\n",
       " '@criticalpath Such an awesome idea - the  continual learning program with a Kindle2  http://bit.ly/1ZLfF',\n",
       " 'RT @justindavey: RT @tweetmeme GM OnStar now instantly sends accident location coordinates to 911 | GPS Obsessed http://bit.ly/16szL1',\n",
       " 'Boarding plane for San Francisco in 1 hour; 6 hr flight. Blech.',\n",
       " 'RT @clashmore: http://bit.ly/SOYv7  Great article by Malcolm Gladwell.',\n",
       " \"I'm really loving the new search site Wolfram/Alpha. Makes Google seem so ... quaint. http://www72.wolframalpha.com/\",\n",
       " 'New blog post: Nike Zoom LeBron Soldier 3 (III) - White / Black - Teal http://bit.ly/rouUS',\n",
       " 'New blog post: Nike Trainer 1 http://bit.ly/394bp',\n",
       " \"@Fraggle312 oh those are awesome! i so wish they weren't owned by nike :(\",\n",
       " '#MBA Admissions Tips Stanford GSB Deadlines and Essay Topics 2009-2010 http://tinyurl.com/pet4fd',\n",
       " 'Wat the heck is North Korea doing!!??!! They just conducted powerful nuclear tests! Follow the link: http://www.msnbc.msn.com/id/30921379',\n",
       " 'I just realized we three monkeys in the white Obama.Biden,Pelosi . Sarah Palin 2012',\n",
       " '@mashable I never did thank you for including me in your Top 100 Twitter Authors! You Rock! (&amp; I New Wave :-D) http://bit.ly/EOrFV',\n",
       " 'Learning jQuery 1.3 Book Review - http://cfbloggers.org/?c=30629',\n",
       " 'Adobe CS4 commercial by Goodby Silverstein: http://bit.ly/1aikhF',\n",
       " \"Wow everyone at the Google I/O conference got free G2's with a month of unlimited service\",\n",
       " '@vkerkez dood I got a free google android phone at the I/O conference. The G2!',\n",
       " \"HTML 5 Demos! Lots of great stuff to come! Yes, I'm excited. :) http://htmlfive.appspot.com #io2009 #googleio\",\n",
       " 'Night At The Museum 2? Pretty furkin good.',\n",
       " '@pambeeslyjenna Jenna, I went to see Night At The Museum 2 today and I was so surprised to see three cast members from The Office...',\n",
       " 'Getting ready to go watch Night at the Museum 2.  Dum dum, you give me gum gum!',\n",
       " 'just watched night at the museum 2! so stinkin cute!',\n",
       " \"So, Night at the Museum 2 was AWESOME! Much better than part 1. Next weekend we'll see Up.\",\n",
       " 'I think I may have a new favorite restaurant. On our way to see \"Night at the Museum 2\".',\n",
       " \"UP! was sold out, so i'm seeing Night At The Museum 2. I'm __ years old.\",\n",
       " 'I Will NEVER Buy a Government Motors Vehicle: Until just recently, I drove GM cars. Since 1988, when I bought a .. http://tinyurl.com/lulsw8',\n",
       " 'Time Warner CEO hints at online fees for magazines      (AP) - Read from Mountain View,United States. Views 16209 http://bit.ly/UdFCH',\n",
       " \"RT @sportsguy33 The upside to Time Warner: unhelpful phone operators   superslow on-site service. Crap, that's not an upside.\",\n",
       " 'RT @sportsguy33: New Time Warner slogan: \"Time Warner, where we make you long for the days before cable.\"',\n",
       " \"confirmed: it's Time Warner's fault, not Facebook's, that fb is taking about 3 minutes to load. so tempted to switch to verizon =/\",\n",
       " 'Weird Piano Guitar House in China! http://u2s.me/72i8',\n",
       " 'Check this video out -- David After Dentist http://bit.ly/47aW2',\n",
       " 'Phillies Driving in the Cadillac with the Top Down in Cali, Win 5-3 - http://tinyurl.com/nzcjqa',\n",
       " '10 tips for healthy eating ? ResultsBy Fitness Blog :: Fitness ... http://bit.ly/62gFn',\n",
       " 'Nike SB Blazer High \"ACG\" Custom - Brad Douglas - http://timesurl.at/45a448',\n",
       " \"I can't watch TV without a Tivo.  And after all these years, the Time/Warner DVR  STILL sucks. http://www.davehitt.com/march03/twdvr.html\",\n",
       " 'I still love my Kindle2 but reading The New York Times on it does not feel natural. I miss the Bloomingdale ads.',\n",
       " 'I love my Kindle2. No more stacks of books to trip over on the way to the loo.',\n",
       " '@Plip Where did you read about tethering support Phil?  Just AT&amp;T or will O2 be joining in?',\n",
       " \"GOT MY WAVE SANDBOX INVITE! Extra excited! Too bad I have class now... but I'll play with it soon enough! #io2009 #wave\",\n",
       " 'I hope the girl at work  buys my Kindle2',\n",
       " \"Missed this insight-filled May column: One smart guy looking closely at why he's impressed with Kindle2 http://bit.ly/i0peY @wroush\",\n",
       " '@sklososky Thanks so much!!! ...from one of your *very* happy Kindle2 winners ; ) I was so surprised, fabulous. Thank you! Best, Kathleen',\n",
       " '@cwong08 I have a Kindle2 (&amp; Sony PRS-500). Like it! Physical device feels good. Font is nice. Pg turns are snappy enuf. UI a little klunky.',\n",
       " 'The #Kindle2 seems the best eReader, but will it work in the UK and where can I get one?',\n",
       " 'I have a google addiction. Thank you for pointing that out, @annamartin123. Hahaha.',\n",
       " \"7 hours. 7 hours of inkscape crashing, normally solid as a rock. 7 hours of LaTeX complaining at the slightest thing. I can't take any more.\",\n",
       " 'Twitter Stock buzz: $AAPL $ES_F $SPY $SPX $PALM  (updated: 12:00 PM)',\n",
       " 'Ask Programming: LaTeX or InDesign?: submitted by calcio1 [link] [1 comment] http://tinyurl.com/myfmf7',\n",
       " 'Ahhh... back in a *real* text editing environment. I &lt;3 LaTeX.']"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweets_df[tweets_df[\"tweet\"].str.contains(r'\\b^')][\"tweet\"].tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Find all @ Mentions\n",
    "We'll use the `\\w` character class to match for mentions (ie. `@ychennay`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0       True\n",
       "1      False\n",
       "2      False\n",
       "3      False\n",
       "4      False\n",
       "       ...  \n",
       "493    False\n",
       "494    False\n",
       "495    False\n",
       "496    False\n",
       "497    False\n",
       "Name: tweet, Length: 498, dtype: bool"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweets_df[\"tweet\"].str.findall(r'@\\w+').apply(lambda x: \"@stellargirl\" in x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Alternative Method to Work With Lists in Pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Quantifiers\n",
    "\n",
    "Quantifiers let you specify how many times a character or group should be matched.\n",
    "![Quantifiers](images/quantifiers.png \"Regex character classes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['aaaaaaaaaaa', 'aaaaaaa']"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = \"aaaaaaaaaaabbbbbbaaaaaaa\"\n",
    "import re\n",
    "\n",
    "re.findall(r'a{3,}', text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"my mom and my dad\"\n",
    "\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['michael', 'christy']"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "email_address = \"yuchen@gmail9com michael@yahoo.com christy@usc.edu\"\n",
    "\n",
    "re.findall(r'(\\w+)@\\w+\\.(?:com|edu)', email_address)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['mom', 'dad']"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "re.findall(r'(mom|dad)', text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "solution2": "shown",
    "solution2_first": true
   },
   "source": [
    "### Match All Phone Numbers ([Link](https://regexr.com/50v17))\n",
    "In the unwanted callers dataset (`unwanted_calls.csv`), parse out all phone numbers.\n",
    "\n",
    "We'll only consider for the time being phone numbers that follow the format `123-456-7890`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "solution2": "shown"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "solution2": "shown",
    "solution2_first": true
   },
   "source": [
    "### Parse Out Zip Codes ([Link](https://regexr.com/50v1g))\n",
    "In the `location_center_point_of_the_zip_code` field, we store both zip codes as well as geolocation data (latitudes and longitudes). We'll only consider zip codes with 5 digits (not the +4 digit delivery route)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "solution2": "shown"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Capture Groups\n",
    "\n",
    "[Oracle documentation on Capture Groups:](https://docs.oracle.com/javase/tutorial/essential/regex/groups.html)\n",
    "> Capturing groups are a way to treat **multiple characters as a single unit**. They are created by placing the characters to be grouped inside a set of parentheses. For example, the regular expression `(dog)` creates a single group containing the letters `d`, `o`, and `g`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example: Parsing out Weekday from Timestamp String ([Link](https://regexr.com/50v0l))\n",
    "\n",
    "In the `date` field of `tweets_df`, we have timestamp strings that look like this: `Mon May 11 03:22:30 UTC 2009`. We want to parse out the weekday from this string."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example: Parsing Out Domain Names\n",
    "We want to capture the domain names of different websites. Here, we need to escape the `.` part of `www.google.com`. In regex, `.` means \"anything\". To actually indicate we want to match for the literal `.` period character, we need to escape it, using `\\.`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "websites = open(\"list_of_websites.txt\").readlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Non-Capture Groups"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sometimes you want to group multiple characters into a single unit to apply regex operations on them, but you don't\n",
    "want to actually capture or return their result."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example 1 [(Link)](https://regexr.com/50t7c)\n",
    "Using non-capture groups to match for optional text:\n",
    "\n",
    "You want to capture both `child` and `children` in your text:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['', 'ren']"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = \"The word child is singular, but the word children is plural. \"\n",
    "\n",
    "re.findall(r'\\bchild(ren)?\\b', text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example 2 [(Link)](https://regexr.com/50t7c)\n",
    "Find all the mentions in the tweets. Mentions start with `@`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example 3 ([Link](https://regexr.com/50ush))\n",
    "Here we want to match for the dollar amount, but we don't want to include the currency notation (`$` or `USD`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"Average fast food wage is $9.08, but inflation has increased USD9.23\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "solution2": "shown"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercises\n",
    "\n",
    "You'll be using the the `tweets_df` Pandas dataframe and `fraudulent_emails.txt` text files for these examples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "email_text = open(\"fraudulent_emails.txt\").read()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Find the subject headings for these emails.\n",
    "**Hint**: Look for the subject line within the email."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Find all hashtags mentioned in the `tweets_df` dataset. Store it as a separate column called **hashtags**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### In `fraudulent_emails.txt`, identify the **list of email addresses** for your security administrator to blacklist from your company's email servers.\n",
    "\n",
    "* Not all emails are malicious! Provide only the list of email addresses from where the email originates from. **Hint**: identify the pattern in the emails that tells you the source of the email.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### In `fraudulent_emails.txt`, identify any IP addresses that should be blacklisted\n",
    "\n",
    "An IPv4 address goes from **1.1.1.1 to 255.255.255.255**. For now, just worry about the number of digits, not whether the value in the address is above `255`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The word \"AT&T\" is not spelled correctly in the `tweets_df` dataset. Correct the misspelling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Removing hashtags and mentions, which topic has the highest average tweet character count?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "solution2": "hidden"
   },
   "outputs": [],
   "source": [
    "tweets_df = tweets_df[\"tweet\"].str.replace(r'AT&amp;', 'AT&')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "solution2": "hidden"
   },
   "outputs": [],
   "source": [
    "# first, we'll find the rows that contain AT&T to see what the pattern looks like\n",
    "tweets_df[tweets_df[\"tweet\"].str.contains(r'(AT)')].head()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "225px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
