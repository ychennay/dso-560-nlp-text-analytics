{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tuning Count Vectorization - One Hot Encoding and other Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "plots_df = pd.read_csv(\"../datasets/movie_plots.csv\")\n",
    "\n",
    "# filter only for American movies\n",
    "plots_df = plots_df[plots_df[\"Origin/Ethnicity\"] == \"American\"]\n",
    "\n",
    " # traditional CountVectorizer\n",
    "vectorizer = CountVectorizer()\n",
    "\n",
    " # use English stopwords, and use one-hot encoding\n",
    "vectorizer = CountVectorizer(stop_words=\"english\", binary=True)\n",
    "\n",
    "# use English stopwords, and use one-hot encoding, and the word must appear in at least two of the movie plots\n",
    "vectorizer = CountVectorizer(stop_words=\"english\", binary=True, min_df=0.05) \n",
    "\n",
    "# use English stopwords, and use one-hot encoding, and the word must appear in at least two of the movie plots\n",
    "# and keep only the top 200\n",
    "vectorizer = CountVectorizer(stop_words=\"english\", binary=True, min_df=2, max_features=200) \n",
    "\n",
    "# use English stopwords, and use one-hot encoding, and the word must appear in at least two of the movie plots\n",
    "# and keep only the top 200\n",
    "vectorizer = CountVectorizer(stop_words=\"english\", binary=True, min_df=2, max_features=200) \n",
    "\n",
    "X = vectorizer.fit_transform(plots_df[\"Plot\"])\n",
    "\n",
    "vectorized_df = pd.DataFrame(X.toarray(), columns=vectorizer.get_feature_names())\n",
    "print(f\"Shape of dataframe is {vectorized_df.shape}\")\n",
    "print(f\"Total number of occurences: {vectorized_df.sum().sum()}\")\n",
    "#print(f\"Word counts: {vectorized_df.sum()}\")\n",
    "vectorized_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cosine Similarity Example\n",
    "\n",
    "### Intro to Algorithmic Marketing:\n",
    "![alt text](../images/cos-sim-textbook1.png \"Logo Title Text 1\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Finding Magnitude of a Vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import numpy as np\n",
    "def magnitude(x): \n",
    "    return math.sqrt(sum(i**2 for i in x))\n",
    "\n",
    "vectorA = [0,3,1,2]\n",
    "\n",
    "print(f\"First approach: {magnitude(vectorA)}\")\n",
    "print(f\"Second approach: {np.linalg.norm(vectorA)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pointwise Mutual Information\n",
    "\n",
    "It's important to identify a **context window** when analyzing co-occurence. In the image below, the context window size is 4 (2 tokens to either side of the target word):\n",
    "\n",
    "![alt text](https://raw.githubusercontent.com/ychennay/dso-560-nlp-text-analytics/main/images/context_window.png \"Logo Title Text 1\")\n",
    "\n",
    "For the purposes of the next section, we'll define the **entire document as the context window.**\n",
    "\n",
    "Pointwise mutual information measures the ratio between the **joint probability of two events happening** with the probabilities of the two events happening, assuming they are independent. It can be defined with the following equation:\n",
    "\n",
    "$$\n",
    "PMI_{A,B} = log\\frac{p(A,B)}{p(A)p(B)}\n",
    "$$\n",
    "\n",
    "Remember that when two events are independent, $P(i,j) = P(i)P(j)$. Using PMI to just a raw word count is often preferable because very common words have extreme skew (\"the\" and \"of\" will co-occur frequently in the same  )\n",
    "\n",
    "```python\n",
    "import math\n",
    "def pmi(tokenA, tokenB, documents, word_counts):\n",
    "    \n",
    "    # word_counts[token_A] => number of times tokenA appears in the documents\n",
    "    # float(len(documents)) => number of documents\n",
    "    # bigram_freq => a dictionary of the number of times tokenA and tokenB are in the same document together\n",
    "    \n",
    "    prob_A = word_counts[tokenA] / float(len(documents))\n",
    "    prob_B = word_counts[tokenB] / float(len(documents))\n",
    "    prob_A_B = bigram_freq[\" \".join([tokenA, tokenB])] / float(len(documents))\n",
    "    return math.log(prob_A_B/float(prob_A*prob_B),2) \n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Collocation\n",
    "\n",
    "Many times, in previous homeworks, we've had to manually try to find phrases that belong together. For example, `New York City`.\n",
    "\n",
    "From [nltk.org](http://www.nltk.org/howto/collocations.html), **collocation** can be defined as\n",
    "\n",
    "> expressions of multiple words which commonly co-occur together. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.collocations import BigramCollocationFinder, BigramAssocMeasures\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk import word_tokenize\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "from nltk.corpus import stopwords\n",
    "stopwords = set(stopwords.words('english') + [\".\",'.', \",\",\":\", \"''\", \"'s\", \"'\", \"``\", \"(\", \")\", \"-\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "documents = []\n",
    "articles = [f\"../datasets/bbcsport/football/00{i}.txt\" for i in range(1,10)]\n",
    "\n",
    "for article in articles:\n",
    "    article = open(article) # open each sports article\n",
    "    for line in article.readlines():\n",
    "        line = line.replace(\"\\n\", \"\") # replace the new line escape character\n",
    "        if len(line) > 0: # if the line is not empty, process it\n",
    "            line = [lemmatizer.lemmatize(token) for token in word_tokenize(line)] \n",
    "            documents.append(line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_documents = []\n",
    "for doc in documents:\n",
    "    new_document = []\n",
    "    for word in doc:\n",
    "        if word.strip().lower() not in stopwords:\n",
    "            new_document.append(word)\n",
    "    new_documents.append(new_document)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "collocation_finder = BigramCollocationFinder.from_documents(new_documents)\n",
    "measures = BigramAssocMeasures()\n",
    "\n",
    "collocation_finder.nbest(measures.raw_freq, 15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Term Frequency / Inverse Document Frequency\n",
    "\n",
    "\n",
    "## Term Frequency\n",
    "![alt text](https://raw.githubusercontent.com/ychennay/dso-560-nlp-text-analytics/main/images/tf-idf1.png \"Term Frequency\")\n",
    "\n",
    "## Inverse Document Frequency\n",
    "![alt text](https://raw.githubusercontent.com/ychennay/dso-560-nlp-text-analytics/main/images/tf-idf2.png \"Inverse Document Frequency\")\n",
    "\n",
    "### Example Calculation\n",
    "\n",
    "![alt text](https://raw.githubusercontent.com/ychennay/dso-560-nlp-text-analytics/main/images/tf-idf4.png \"Example\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using Scikit-Learn to Generate TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "vectorizer = TfidfVectorizer(ngram_range=(3,4),\n",
    "                             token_pattern=r'\\b[a-zA-Z]{3,}\\b',\n",
    "                             max_df=0.4, stop_words=stopwords.words())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"mcdonalds-yelp-negative-reviews.csv\", encoding=\"latin1\")\n",
    "corpus = list(df[\"review\"].values)\n",
    "\n",
    "X = vectorizer.fit_transform(corpus)\n",
    "terms = vectorizer.get_feature_names()\n",
    "tf_idf = pd.DataFrame(X.toarray().transpose(), index=terms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf_idf = tf_idf.sum(axis=1)\n",
    "score = pd.DataFrame(tf_idf, columns=[\"score\"])\n",
    "score.sort_values(by=\"score\", ascending=False, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "score.to_csv(\"scores.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercises\n",
    "\n",
    "For the following exercises, use the definitions below:\n",
    "\n",
    "**Term frequency**:\n",
    "$$\n",
    "tf = n(t,d)\n",
    "$$\n",
    "**Inverse document frequency**:\n",
    "$$\n",
    "idf = 1 + \\frac{N}{df(t) + 1}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "documents = [\n",
    "    \"He ate the food\",\n",
    "    \"He liked the meal\",\n",
    "    \"She likes the food from McDonalds, but she avoids the food from Burger King\",\n",
    "    \"They like to eat 3 meals a day\"\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculate the TF-IDF score for `like` in each of the documents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculate the TF-IDF score for `the food` bigram in each of the documents"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
