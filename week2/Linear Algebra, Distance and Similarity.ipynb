{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#Linear-Algebra\" data-toc-modified-id=\"Linear-Algebra-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>Linear Algebra</a></span><ul class=\"toc-item\"><li><span><a href=\"#Dot-Products\" data-toc-modified-id=\"Dot-Products-1.1\"><span class=\"toc-item-num\">1.1&nbsp;&nbsp;</span>Dot Products</a></span><ul class=\"toc-item\"><li><span><a href=\"#What-does-a-dot-product-conceptually-mean?\" data-toc-modified-id=\"What-does-a-dot-product-conceptually-mean?-1.1.1\"><span class=\"toc-item-num\">1.1.1&nbsp;&nbsp;</span>What does a dot product conceptually mean?</a></span></li></ul></li><li><span><a href=\"#Exercises\" data-toc-modified-id=\"Exercises-1.2\"><span class=\"toc-item-num\">1.2&nbsp;&nbsp;</span>Exercises</a></span></li><li><span><a href=\"#Using-Scikit-Learn\" data-toc-modified-id=\"Using-Scikit-Learn-1.3\"><span class=\"toc-item-num\">1.3&nbsp;&nbsp;</span>Using Scikit-Learn</a></span></li><li><span><a href=\"#Bag-of-Words-Models\" data-toc-modified-id=\"Bag-of-Words-Models-1.4\"><span class=\"toc-item-num\">1.4&nbsp;&nbsp;</span>Bag of Words Models</a></span></li></ul></li><li><span><a href=\"#Distance-Measures\" data-toc-modified-id=\"Distance-Measures-2\"><span class=\"toc-item-num\">2&nbsp;&nbsp;</span>Distance Measures</a></span><ul class=\"toc-item\"><li><span><a href=\"#Euclidean-Distance\" data-toc-modified-id=\"Euclidean-Distance-2.1\"><span class=\"toc-item-num\">2.1&nbsp;&nbsp;</span>Euclidean Distance</a></span><ul class=\"toc-item\"><li><span><a href=\"#Scikit-Learn\" data-toc-modified-id=\"Scikit-Learn-2.1.1\"><span class=\"toc-item-num\">2.1.1&nbsp;&nbsp;</span>Scikit Learn</a></span></li></ul></li></ul></li><li><span><a href=\"#Similarity-Measures\" data-toc-modified-id=\"Similarity-Measures-3\"><span class=\"toc-item-num\">3&nbsp;&nbsp;</span>Similarity Measures</a></span></li><li><span><a href=\"#Linear-Relationships\" data-toc-modified-id=\"Linear-Relationships-4\"><span class=\"toc-item-num\">4&nbsp;&nbsp;</span>Linear Relationships</a></span><ul class=\"toc-item\"><li><span><a href=\"#Pearson-Correlation-Coefficient\" data-toc-modified-id=\"Pearson-Correlation-Coefficient-4.1\"><span class=\"toc-item-num\">4.1&nbsp;&nbsp;</span>Pearson Correlation Coefficient</a></span><ul class=\"toc-item\"><li><span><a href=\"#Intuition-Behind-Pearson-Correlation-Coefficient\" data-toc-modified-id=\"Intuition-Behind-Pearson-Correlation-Coefficient-4.1.1\"><span class=\"toc-item-num\">4.1.1&nbsp;&nbsp;</span>Intuition Behind Pearson Correlation Coefficient</a></span><ul class=\"toc-item\"><li><span><a href=\"#When-$ρ_{Χ_Υ}-=-1$-or--$ρ_{Χ_Υ}-=--1$\" data-toc-modified-id=\"When-$ρ_{Χ_Υ}-=-1$-or--$ρ_{Χ_Υ}-=--1$-4.1.1.1\"><span class=\"toc-item-num\">4.1.1.1&nbsp;&nbsp;</span>When $ρ_{Χ_Υ} = 1$ or  $ρ_{Χ_Υ} = -1$</a></span></li></ul></li></ul></li><li><span><a href=\"#Cosine-Similarity\" data-toc-modified-id=\"Cosine-Similarity-4.2\"><span class=\"toc-item-num\">4.2&nbsp;&nbsp;</span>Cosine Similarity</a></span><ul class=\"toc-item\"><li><span><a href=\"#Shift-Invariance\" data-toc-modified-id=\"Shift-Invariance-4.2.1\"><span class=\"toc-item-num\">4.2.1&nbsp;&nbsp;</span>Shift Invariance</a></span></li></ul></li></ul></li><li><span><a href=\"#Exercise-(20-minutes):\" data-toc-modified-id=\"Exercise-(20-minutes):-5\"><span class=\"toc-item-num\">5&nbsp;&nbsp;</span><span style=\"background-color: #ffff00\">Exercise (20 minutes):</span></a></span><ul class=\"toc-item\"><li><ul class=\"toc-item\"><li><ul class=\"toc-item\"><li><span><a href=\"#3.-Define-your-cosine-similarity-functions\" data-toc-modified-id=\"3.-Define-your-cosine-similarity-functions-5.0.0.1\"><span class=\"toc-item-num\">5.0.0.1&nbsp;&nbsp;</span>3. Define your cosine similarity functions</a></span></li><li><span><a href=\"#4.-Get-the-two-documents-from-the-BoW-feature-space-and-calculate-cosine-similarity\" data-toc-modified-id=\"4.-Get-the-two-documents-from-the-BoW-feature-space-and-calculate-cosine-similarity-5.0.0.2\"><span class=\"toc-item-num\">5.0.0.2&nbsp;&nbsp;</span>4. Get the two documents from the BoW feature space and calculate cosine similarity</a></span></li></ul></li></ul></li></ul></li><li><span><a href=\"#Challenge:-Use-the-Example-Below-to-Create-Your-Own-Cosine-Similarity-Function\" data-toc-modified-id=\"Challenge:-Use-the-Example-Below-to-Create-Your-Own-Cosine-Similarity-Function-6\"><span class=\"toc-item-num\">6&nbsp;&nbsp;</span>Challenge: Use the Example Below to Create Your Own Cosine Similarity Function</a></span><ul class=\"toc-item\"><li><ul class=\"toc-item\"><li><span><a href=\"#Create-a-list-of-all-the-vocabulary-$V$\" data-toc-modified-id=\"Create-a-list-of-all-the-vocabulary-$V$-6.0.1\"><span class=\"toc-item-num\">6.0.1&nbsp;&nbsp;</span>Create a list of all the <strong>vocabulary $V$</strong></a></span><ul class=\"toc-item\"><li><span><a href=\"#Native-Implementation:\" data-toc-modified-id=\"Native-Implementation:-6.0.1.1\"><span class=\"toc-item-num\">6.0.1.1&nbsp;&nbsp;</span>Native Implementation:</a></span></li></ul></li><li><span><a href=\"#Create-your-Bag-of-Words-model\" data-toc-modified-id=\"Create-your-Bag-of-Words-model-6.0.2\"><span class=\"toc-item-num\">6.0.2&nbsp;&nbsp;</span>Create your Bag of Words model</a></span></li></ul></li></ul></li></ul></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear Algebra\n",
    "\n",
    "In the natural language processing, each document is a vector of numbers.\n",
    "\n",
    "\n",
    "## Dot Products\n",
    "\n",
    "A dot product is defined as\n",
    "\n",
    "$ a \\cdot b = \\sum_{i}^{n} a_{i}b_{i} = a_{1}b_{1} + a_{2}b_{2} + a_{3}b_{3} + \\dots + a_{n}b_{n}$\n",
    "\n",
    "The geometric definition of a dot product is \n",
    "\n",
    "$ a \\cdot b = $\\|\\|b\\|\\|\\|\\|a\\|\\|\n",
    "\n",
    "### What does a dot product conceptually mean?\n",
    "\n",
    "A dot product is a representation of the **similarity between two components**, because it is calculated based upon shared elements. It tells you how much one vector goes in the direction of another vector.\n",
    "\n",
    "The actual value of a dot product reflects the direction of change:\n",
    "\n",
    "* **Zero**: we don't have any growth in the original direction\n",
    "* **Positive** number: we have some growth in the original direction\n",
    "* **Negative** number: we have negative (reverse) growth in the original direction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "A = [0,2]\n",
    "B = [0,1]\n",
    "\n",
    "def dot_product(x,y):\n",
    "    return sum(a*b for a,b in zip(x,y))\n",
    "\n",
    "dot_product(A,B)\n",
    "# What will the dot product of A and B be?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Correlations](images/dot_product.png \"Visualization of various r values for Pearson correlation coefficient\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercises"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What will the dot product of `A` and `B` be?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "A = [1,2]\n",
    "B = [2,4]\n",
    "dot_product(A,B)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What will the dot product of `document_1` and `document_2` be?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "document_1 = [0, 0, 1]\n",
    "document_2 = [1, 0, 2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using Scikit-Learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bag of Words Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = [\n",
    "        \"Some analysts think demand could drop this year because a large number of homeowners take on remodeling projectsafter buying a new property. With fewer homes selling, home values easing, and mortgage rates rising, they predict home renovations could fall to their lowest levels in three years.\", \n",
    "    \n",
    "          \"Most home improvement stocks are expected to report fourth-quarter earnings next month.\",\n",
    "    \n",
    "         \"The conversation boils down to how much leverage management can get out of its wide-ranging efforts to re-energize operations, branding, digital capabilities, and the menu–and, for investors, how much to pay for that.\",\n",
    "    \n",
    "    \"RMD’s software acquisitions, efficiency, and mix overcame pricing and its gross margin improved by 90 bps Y/Y while its operating margin (including amortization) improved by 80 bps Y/Y. Since RMD expects the slower international flow generator growth to continue for the next few quarters, we have lowered our organic growth estimates to the mid-single digits. \"\n",
    "]\n",
    "\n",
    "X = vectorizer.fit_transform(corpus).toarray() \n",
    "import numpy as np\n",
    "from sys import getsizeof\n",
    "\n",
    "zeroes = np.where(X.flatten() == 0)[0].size \n",
    "percent_sparse = zeroes / X.size\n",
    "print(f\"The bag of words feature space is {round(percent_sparse * 100,2)}% sparse. \\n\\\n",
    "That's approximately {round(getsizeof(X) * percent_sparse,2)} bytes of wasted memory. This is why sklearn uses CSR (compressed sparse rows) instead of normal matrices!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Distance Measures\n",
    "\n",
    "\n",
    "## Euclidean Distance\n",
    "\n",
    "Euclidean distances can range from 0 (completely identically) to $\\infty$ (extremely dissimilar). \n",
    "\n",
    "The distance between two points, $x$ and $y$, can be defined as $d(x,y)$:\n",
    "\n",
    "$$\n",
    "d(x,y) = \\sqrt{\\sum_{i=1}^{n}(x_{i}-y_{i})^2}\n",
    "$$\n",
    "\n",
    "Compared to the other dominant distance measure (cosine similarity), **magnitude** plays an extremely important role."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from math import sqrt\n",
    " \n",
    "def euclidean_distance_1(x,y):\n",
    "    distance = sum((a-b)**2 for a, b in zip(x, y))\n",
    "    return sqrt(distance)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There's typically an easier way to write this function that takes advantage of Numpy's vectorization capabilities:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "def euclidean_distance_2(x,y):\n",
    "    x = np.array(x)\n",
    "    y = np.array(y)\n",
    "    return np.linalg.norm(x-y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scikit Learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import euclidean_distances\n",
    "X = [document_1, document_2]\n",
    "euclidean_distances(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Similarity Measures\n",
    "\n",
    "Similarity measures will always range between -1 and 1. A similarity of -1 means the two objects are complete opposites, while a similarity of 1 indicates the objects are identical.\n",
    "\n",
    "\n",
    "# Linear Relationships\n",
    "\n",
    "## Pearson Correlation Coefficient\n",
    "* We use **ρ** when the correlation is being measured from the population, and **r** when it is being generated from a sample.\n",
    "* An r value of 1 represents a **perfect linear** relationship, and a value of -1 represents a perfect inverse linear relationship.\n",
    "\n",
    "The equation for Pearson's correlation coefficient is \n",
    "$$\n",
    "ρ_{Χ_Υ} = \\frac{cov(X,Y)}{σ_Xσ_Y}\n",
    "$$\n",
    "\n",
    "### Intuition Behind Pearson Correlation Coefficient\n",
    "\n",
    "#### When $ρ_{Χ_Υ} = 1$ or  $ρ_{Χ_Υ} = -1$\n",
    "\n",
    "This requires **$cov(X,Y) = σ_Xσ_Y$** or **$-1 * cov(X,Y) = σ_Xσ_Y$** (in the case of $ρ = -1$) . This corresponds with all the data points lying perfectly on the same line.\n",
    "![Correlations](images/correlation.png \"Visualization of various r values for Pearson correlation coefficient\")\n",
    "\n",
    "\n",
    "## Cosine Similarity\n",
    "\n",
    "The cosine similarity of two vectors (each vector will usually represent one document) is a measure that calculates $ cos(\\theta)$, where $\\theta$ is the angle between the two vectors.\n",
    "\n",
    "Therefore, if the vectors are **orthogonal** to each other (90 degrees), $cos(90) = 0$. If the vectors are in exactly the same direction, $\\theta = 0$ and $cos(0) = 1$.\n",
    "\n",
    "Cosine similiarity **does not care about the magnitude of the vector, only the direction** in which it points. This can help normalize when comparing across documents that are different in terms of word count.\n",
    "\n",
    "![Cosine Similarity](images/cos-equation.png)\n",
    "\n",
    "### Shift Invariance\n",
    "\n",
    "* The Pearson correlation coefficient between X and Y does not change with you transform $X \\rightarrow a + bX$ and $Y \\rightarrow c + dY$, assuming $a$, $b$, $c$, and $d$ are constants and $b$ and $d$ are positive.\n",
    "* Cosine similarity does, however, change when transformed in this way.\n",
    "\n",
    "\n",
    "<h1><span style=\"background-color: #FFFF00\">Exercise (20 minutes):</span></h1>\n",
    "\n",
    ">In Python, find the **cosine similarity** and the **Pearson correlation coefficient** of the two following sentences, assuming a **one-hot encoded binary bag of words** model. You may use a library to create the BoW feature space, but do not use libraries other than `numpy` or `scipy` to compute Pearson and cosine similarity:\n",
    "\n",
    ">`A = \"John likes to watch movies. Mary likes movies too\"`\n",
    "\n",
    ">`B = \"John also likes to watch football games, but he likes to watch movies on occasion as well\"`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. Define your cosine similarity functions\n",
    "\n",
    "```python\n",
    "from scipy.spatial.distance import cosine # we are importing this library to check that our own cosine similarity func works\n",
    "from numpy import dot # to calculate dot product\n",
    "from numpy.linalg import norm # to calculate the norm\n",
    "\n",
    "def cosine_similarity(A, B):\n",
    "    numerator = dot(A, B)\n",
    "    denominator = norm(A) * norm(B)\n",
    "    return numerator / denominator\n",
    "\n",
    "def cosine_distance(A,B):\n",
    "    return 1 - cosine_similarity\n",
    "\n",
    "A = [0,2,3,4,1,2]\n",
    "B = [1,3,4,0,0,2]\n",
    "\n",
    "# check that your native implementation and 3rd party library function produce the same values\n",
    "assert round(cosine_similarity(A,B),4) ==  round(cosine(A,B),4)\n",
    "```\n",
    "\n",
    "#### 4. Get the two documents from the BoW feature space and calculate cosine similarity\n",
    "\n",
    "```python\n",
    "cosine_similarity(X[0], X[1])\n",
    "```\n",
    ">0.5241424183609592"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.spatial.distance import cosine\n",
    "from numpy import dot\n",
    "import numpy as np\n",
    "from numpy.linalg import norm\n",
    "\n",
    "def cosine_similarity(A, B):\n",
    "    numerator = dot(A, B)\n",
    "    denominator = norm(A) * norm(B)\n",
    "    return numerator / denominator # remember, you take 1 - the distance to get the distance\n",
    "\n",
    "def cosine_distance(A,B):\n",
    "    return 1 - cosine_similarity\n",
    "\n",
    "A = [0,2,3,4,1,2]\n",
    "B = [1,3,4,0,0,2]\n",
    "\n",
    "# check that your native implementation and 3rd party library function produce the same values\n",
    "assert round(cosine_similarity(A,B),4) ==  round(1 - cosine(A,B),4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "vectorizer = CountVectorizer()\n",
    "\n",
    "# take two very similar sentences, should have high similarity\n",
    "# edit these sentences to become less similar, and the similarity score should decrease\n",
    "data_corpus = [\"John likes to watch movies. Mary likes movies too.\", \n",
    "\"John also likes to watch football games\"]\n",
    "\n",
    "X = vectorizer.fit_transform(data_corpus) \n",
    "X = X.toarray()\n",
    "print(vectorizer.get_feature_names())\n",
    "cosine_similarity(X[0], X[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Challenge: Use the Example Below to Create Your Own Cosine Similarity Function\n",
    "\n",
    "### Create a list of all the **vocabulary $V$**\n",
    "\n",
    "Using **`sklearn`**'s **`CountVectorizer`**:\n",
    "```python\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "vectorizer = CountVectorizer()\n",
    "data_corpus = [\"John likes to watch movies. Mary likes movies too\", \n",
    "\"John also likes to watch football games, but he likes to watch movies on occasion as well\"]\n",
    "X = vectorizer.fit_transform(data_corpus) \n",
    "V = vectorizer.get_feature_names()\n",
    "```\n",
    "\n",
    "#### Native Implementation:\n",
    "```python\n",
    "def get_vocabulary(sentences):\n",
    "    vocabulary = {} # create an empty set - question: Why not a list?\n",
    "    for sentence in sentences:\n",
    "         # this is a very crude form of \"tokenization\", would not actually use in production\n",
    "        for word in sentence.split(\" \"):\n",
    "            if word not in vocabulary:\n",
    "                vocabulary.add(word)\n",
    "    return vocabulary\n",
    "```\n",
    "\n",
    "### Create your Bag of Words model\n",
    "```python\n",
    "X = X.toarray()\n",
    "print(X)\n",
    "```\n",
    "Your console output:\n",
    "```python\n",
    "[[0 0 0 1 2 1 2 1 1 1]\n",
    " [1 1 1 1 1 0 0 1 0 1]]\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectors = [[0,0,0,1,2,1,2,1,1,1],\n",
    "           [1,1,1,1,1,0,0,1,0,1]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "def find_norm(vector):\n",
    "    total = 0\n",
    "    for element in vector:\n",
    "        total += element ** 2\n",
    "        \n",
    "    return math.sqrt(total)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "norm(vectors[0]) # Numpy\n",
    "find_norm(vectors[0]) # your own"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dot_product(vectors[0], vectors[1]) / (find_norm(vectors[0]) * find_norm(vectors[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import cosine_distances, cosine_similarity\n",
    "cosine_similarity(vectors)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
