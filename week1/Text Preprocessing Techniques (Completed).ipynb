{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#Setup\" data-toc-modified-id=\"Setup-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>Setup</a></span></li><li><span><a href=\"#Information-Retrieval\" data-toc-modified-id=\"Information-Retrieval-2\"><span class=\"toc-item-num\">2&nbsp;&nbsp;</span>Information Retrieval</a></span><ul class=\"toc-item\"><li><ul class=\"toc-item\"><li><span><a href=\"#Practical-Example\" data-toc-modified-id=\"Practical-Example-2.0.1\"><span class=\"toc-item-num\">2.0.1&nbsp;&nbsp;</span>Practical Example</a></span></li><li><span><a href=\"#Boolean-Search-Queries\" data-toc-modified-id=\"Boolean-Search-Queries-2.0.2\"><span class=\"toc-item-num\">2.0.2&nbsp;&nbsp;</span>Boolean Search Queries</a></span></li><li><span><a href=\"#Examples-of-Documents-That-Would-Satisfy-the-Above-Document-Term-Matrix\" data-toc-modified-id=\"Examples-of-Documents-That-Would-Satisfy-the-Above-Document-Term-Matrix-2.0.3\"><span class=\"toc-item-num\">2.0.3&nbsp;&nbsp;</span>Examples of Documents That Would Satisfy the Above Document-Term Matrix</a></span><ul class=\"toc-item\"><li><span><a href=\"#Query-Example-(nuclear-AND-treaty)\" data-toc-modified-id=\"Query-Example-(nuclear-AND-treaty)-2.0.3.1\"><span class=\"toc-item-num\">2.0.3.1&nbsp;&nbsp;</span>Query Example <code>(nuclear AND treaty)</code></a></span></li><li><span><a href=\"#Query-Example-(nuclear-AND-treaty)-OR-((NOT-treaty)-AND-(nonproliferation-OR-Iran))\" data-toc-modified-id=\"Query-Example-(nuclear-AND-treaty)-OR-((NOT-treaty)-AND-(nonproliferation-OR-Iran))-2.0.3.2\"><span class=\"toc-item-num\">2.0.3.2&nbsp;&nbsp;</span>Query Example <code>(nuclear AND treaty) OR ((NOT treaty) AND (nonproliferation OR Iran))</code></a></span></li></ul></li></ul></li></ul></li><li><span><a href=\"#Tokenizing-Sentences\" data-toc-modified-id=\"Tokenizing-Sentences-3\"><span class=\"toc-item-num\">3&nbsp;&nbsp;</span>Tokenizing Sentences</a></span><ul class=\"toc-item\"><li><span><a href=\"#Definition-of-a-Token\" data-toc-modified-id=\"Definition-of-a-Token-3.1\"><span class=\"toc-item-num\">3.1&nbsp;&nbsp;</span>Definition of a Token</a></span></li><li><span><a href=\"#Examples-of-Tokens/Words-Not-Being-the-Identical-Concepts\" data-toc-modified-id=\"Examples-of-Tokens/Words-Not-Being-the-Identical-Concepts-3.2\"><span class=\"toc-item-num\">3.2&nbsp;&nbsp;</span>Examples of Tokens/Words Not Being the Identical Concepts</a></span></li><li><span><a href=\"#Definition-of-a-Document\" data-toc-modified-id=\"Definition-of-a-Document-3.3\"><span class=\"toc-item-num\">3.3&nbsp;&nbsp;</span>Definition of a Document</a></span></li><li><span><a href=\"#Why-Is-Tokenization-Hard?\" data-toc-modified-id=\"Why-Is-Tokenization-Hard?-3.4\"><span class=\"toc-item-num\">3.4&nbsp;&nbsp;</span>Why Is Tokenization Hard?</a></span><ul class=\"toc-item\"><li><span><a href=\"#Sentence-Boundary-Detection\" data-toc-modified-id=\"Sentence-Boundary-Detection-3.4.1\"><span class=\"toc-item-num\">3.4.1&nbsp;&nbsp;</span>Sentence Boundary Detection</a></span></li><li><span><a href=\"#Approaches-to-Sentence-Boundary-Detection\" data-toc-modified-id=\"Approaches-to-Sentence-Boundary-Detection-3.4.2\"><span class=\"toc-item-num\">3.4.2&nbsp;&nbsp;</span>Approaches to Sentence Boundary Detection</a></span><ul class=\"toc-item\"><li><span><a href=\"#Rules-Based\" data-toc-modified-id=\"Rules-Based-3.4.2.1\"><span class=\"toc-item-num\">3.4.2.1&nbsp;&nbsp;</span>Rules-Based</a></span></li><li><span><a href=\"#Supervised/Unsupervised-Learning\" data-toc-modified-id=\"Supervised/Unsupervised-Learning-3.4.2.2\"><span class=\"toc-item-num\">3.4.2.2&nbsp;&nbsp;</span>Supervised/Unsupervised Learning</a></span></li></ul></li><li><span><a href=\"#NLTK's-PunktSentenceTokenizer\" data-toc-modified-id=\"NLTK's-PunktSentenceTokenizer-3.4.3\"><span class=\"toc-item-num\">3.4.3&nbsp;&nbsp;</span>NLTK's <code>PunktSentenceTokenizer</code></a></span></li></ul></li><li><span><a href=\"#Stemming\" data-toc-modified-id=\"Stemming-3.5\"><span class=\"toc-item-num\">3.5&nbsp;&nbsp;</span>Stemming</a></span></li><li><span><a href=\"#Lemmatization\" data-toc-modified-id=\"Lemmatization-3.6\"><span class=\"toc-item-num\">3.6&nbsp;&nbsp;</span>Lemmatization</a></span><ul class=\"toc-item\"><li><span><a href=\"#Why-Stemming?\" data-toc-modified-id=\"Why-Stemming?-3.6.1\"><span class=\"toc-item-num\">3.6.1&nbsp;&nbsp;</span>Why Stemming?</a></span></li><li><span><a href=\"#Differences-Between-Stemming-and-Lemmatization\" data-toc-modified-id=\"Differences-Between-Stemming-and-Lemmatization-3.6.2\"><span class=\"toc-item-num\">3.6.2&nbsp;&nbsp;</span>Differences Between Stemming and Lemmatization</a></span></li></ul></li><li><span><a href=\"#Scoring-Metrics\" data-toc-modified-id=\"Scoring-Metrics-3.7\"><span class=\"toc-item-num\">3.7&nbsp;&nbsp;</span>Scoring Metrics</a></span><ul class=\"toc-item\"><li><span><a href=\"#Precision/Recall\" data-toc-modified-id=\"Precision/Recall-3.7.1\"><span class=\"toc-item-num\">3.7.1&nbsp;&nbsp;</span>Precision/Recall</a></span></li><li><span><a href=\"#F1-Score\" data-toc-modified-id=\"F1-Score-3.7.2\"><span class=\"toc-item-num\">3.7.2&nbsp;&nbsp;</span>F1 Score</a></span></li><li><span><a href=\"#Exercises\" data-toc-modified-id=\"Exercises-3.7.3\"><span class=\"toc-item-num\">3.7.3&nbsp;&nbsp;</span>Exercises</a></span></li></ul></li><li><span><a href=\"#Removing-Stopwords\" data-toc-modified-id=\"Removing-Stopwords-3.8\"><span class=\"toc-item-num\">3.8&nbsp;&nbsp;</span>Removing Stopwords</a></span><ul class=\"toc-item\"><li><span><a href=\"#CountVectorize-the-Documents\" data-toc-modified-id=\"CountVectorize-the-Documents-3.8.1\"><span class=\"toc-item-num\">3.8.1&nbsp;&nbsp;</span>CountVectorize the Documents</a></span></li><li><span><a href=\"#When-Should-You-Avoid-Removing-Stopwords?\" data-toc-modified-id=\"When-Should-You-Avoid-Removing-Stopwords?-3.8.2\"><span class=\"toc-item-num\">3.8.2&nbsp;&nbsp;</span>When Should You Avoid Removing Stopwords?</a></span></li></ul></li><li><span><a href=\"#Exercise:\" data-toc-modified-id=\"Exercise:-3.9\"><span class=\"toc-item-num\">3.9&nbsp;&nbsp;</span>Exercise:</a></span></li></ul></li><li><span><a href=\"#Vectorization-Techniques\" data-toc-modified-id=\"Vectorization-Techniques-4\"><span class=\"toc-item-num\">4&nbsp;&nbsp;</span>Vectorization Techniques</a></span><ul class=\"toc-item\"><li><span><a href=\"#Count-Vectorization\" data-toc-modified-id=\"Count-Vectorization-4.1\"><span class=\"toc-item-num\">4.1&nbsp;&nbsp;</span>Count Vectorization</a></span></li></ul></li></ul></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You'll need to download/install `nltk`, and then also download several of `nltk`'s modules."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /Users/yuchen/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/yuchen/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt') # A popular NLTK sentence tokenizer\n",
    "nltk.download('stopwords') # library of common English stopwords"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Information Retrieval"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Information retrieval (IR)** is finding material (usually documents) of an unstructured nature (usually text) that **satisfies an information need** from within large collections (usually stored on computers).\n",
    "\n",
    "### Practical Example\n",
    "\n",
    "A mobile app contains a gallery of about 8,000 pieces of artwork that they'd like to sell to users. Each artwork contains artwork name, artist name, descriptions, and some keyword tags. Th"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Boolean Search Queries\n",
    "\n",
    "Example of a **document-term matrix**, from [Northeastern University, Fall 2006 Information Retrieval](http://www.ccs.neu.edu/home/jaa/CSG339.06F/Lectures/boolean.pdf):\n",
    "\n",
    "<img src=\"images/boolean_search_query.png\" alt=\"Different Stemming Techniques\" style=\"width:600px;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Examples of Documents That Would Satisfy the Above Document-Term Matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **Document 1**\n",
    "\n",
    "\"The new cookbook was a huge hit with younger millenials.\"\n",
    "* **Document 2**\n",
    "\n",
    "\"Iran has significant nuclear production capabilities.\"\n",
    "* **Document 3**\n",
    "\n",
    "\"The treaty that ended the American Revolution was signed soon thereafter.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Query Example `(nuclear AND treaty)`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`D7` would get returned since it is the only document that contains both `nuclear` and `treaty`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Query Example `(nuclear AND treaty) OR ((NOT treaty) AND (nonproliferation OR Iran))`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* `D7`: satisfies LHS (left-hand side of expression) - contains `nuclear` and `treaty`\n",
    "* `D5`: satisfied RHS - contains `nonproliferation` and not `treaty`)\n",
    "* `D2`: satisfies RHS - contains `Iran` and not `treaty`)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tokenizing Sentences\n",
    "\n",
    "## Definition of a Token\n",
    "\n",
    "> A token is an instance of a sequence of characters in some particular document that are grouped together as a useful semantic unit for processing. ([Stanford NLP](https://nlp.stanford.edu/IR-book/html/htmledition/tokenization-1.html)):\n",
    "\n",
    "<img src=\"images/tokenization.png\" alt=\"Different Stemming Techniques\" style=\"width:600px;\"/>\n",
    "\n",
    "\n",
    "## Examples of Tokens/Words Not Being the Identical Concepts\n",
    "\n",
    "* Multi-word entities: `San Francisco`, `New York City`\n",
    "* Phone numbers / dates: `(800) 123-4564` and `Apr. 30th, 2020`\n",
    "\n",
    "\n",
    "At first, the task of tokenizing seems simple:\n",
    "1. Split apart `corpus` into `documents`.\n",
    "2. Split apart `documents` into `tokens`.\n",
    "\n",
    "## Definition of a Document\n",
    "\n",
    "A document is a distinct group of tokens. For example, in the Amazon toy product review dataset, each review can be considered a document. In *Tale of Two Cities*, a document could be either a sentence, or a paragraph, or a chapter - it all depends on the task at hand.\n",
    "\n",
    "* **Sentiment Analysis**: each document is a review/comment/tweet\n",
    "* **Authorship Classification** (determining if a piece of text was written by Person A or Person B): the entire text is a document"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Why Is Tokenization Hard?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sentence Boundary Detection\n",
    "\n",
    "Deciding where one sentence ends and another sentence begins is easy for humans, but extremely complex for machines. A prime cause of this is ambiguity of punctuation marks. Particularly in English, periods can mean many things beyond denoting the end of a sentence:\n",
    "* abbreviations\n",
    "* decimal points\n",
    "* domain names and email addresses\n",
    "\n",
    "Furthermore, other \"obvious\" sentence boundaries like question marks and exclamation marks are becoming increasingly ambiguous due to slang, emoticons, or just emphasis (`\"What are you doing???\"` should not be split into 3 separate sentences).\n",
    "\n",
    "### Approaches to Sentence Boundary Detection\n",
    "\n",
    "#### Rules-Based\n",
    "\n",
    "* Using regex: `((?<=[a-z0-9][.?!])|(?<=[a-z0-9][.?!]\\\"))(\\s|\\r\\n)(?=\\\"?[A-Z])` (don't worry if you can't understand this yet- it includes usage of several advanced techniques such as **lookahead/behind references** which we have not covered).\n",
    "\n",
    "#### Supervised/Unsupervised Learning\n",
    "\n",
    "* Maximum entropy models ([UC Berkeley's SATZ Adaptive Sentence Boundary Detector](https://web.archive.org/web/20070922132340/http://elib.cs.berkeley.edu/src/satz/))\n",
    "* NLTK's `Punkt` sentence tokenizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Why not just tokenize myself?\n",
    "text = \"I made two purchases today! I bought a bag of grapes for $4.99, \\\n",
    "but then... realized John Francis already bought some at the Y.M.C.A!\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['I made two purchases today! I bought a bag of grapes for $4',\n",
       " '99, but then',\n",
       " '',\n",
       " '',\n",
       " ' realized John Francis already bought some at the Y',\n",
       " 'M',\n",
       " 'C',\n",
       " 'A!']"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# trying to write our own tokenizer\n",
    "text.split(\".\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What are some of the issues you notice with the above approach?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NLTK's `PunktSentenceTokenizer`\n",
    "\n",
    "> `PunktSentenceTokenizer` is an **sentence boundary detection algorithm** that must be trained to be used. NLTK already includes a pre-trained version of the `PunktSentenceTokenizer` ([StackOverflow](https://stackoverflow.com/questions/35275001/use-of-punktsentencetokenizer-in-nltk))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['I made two purchases today!',\n",
       " 'I bought a bag of grapes for $4.99, but then... realized John Francis already bought some at the Y.M.C.A!']"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Using NLTK sent_tokenize()\n",
    "sent_text = nltk.sent_tokenize(text) # this gives us a list of sentences\n",
    "sent_text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stemming\n",
    "\n",
    "Stemming is the process of reducing inflection in words to their root forms such as mapping a group of words to the same stem even if the stem itself is not a valid word in the language [Source](https://www.datacamp.com/community/tutorials/stemming-lemmatization-python)\n",
    "\n",
    "<img src=\"images/stemming-examples.png\" alt=\"Different Stemming Techniques\" style=\"width:600px;\"/>\n",
    "\n",
    "In Python, we can use **`nltk.stem.porter.PorterStemmer`** stem our words:\n",
    "\n",
    "```python\n",
    "stemmer = PorterStemmer()\n",
    "print(stemmer.stem(\"caressed\"))  # caress\n",
    "print(stemmer.stem(\"athlete\"))  # athlet\n",
    "print(stemmer.stem(\"athletics\"))  # athlet\n",
    "print(stemmer.stem(\"media\"))  # media\n",
    "print(stemmer.stem(\"photography\"))  # photographi\n",
    "print(stemmer.stem(\"sexy\"))  # sexi\n",
    "print(stemmer.stem(\"journalling\"))  # journal\n",
    "print(stemmer.stem(\"Slovakia\")) # slovakia\n",
    "print(stemmer.stem(\"corpora\")) # corpora\n",
    "print(stemmer.stem(\"thieves\")) # thiev\n",
    "print(stemmer.stem(\"rocks\")) # rock\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lemmatization\n",
    "\n",
    "Lemmatization is closely related to stemming. However, while stemming looks only at the individual word itself, and considers the usage of the word (ie. part of speech, is this word a noun, a verb, etc.). For example, if we compared the [outputs of stemming and lemmatization certain ambiguous French words](https://blog.bitext.com/lemmatization-vs-stemming):\n",
    "\n",
    "<img src=\"images/comparison.png\" alt=\"Different Stemming Techniques\" style=\"width:600px;\"/>\n",
    "\n",
    "```python\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "print(lemmatizer.lemmatize(\"caressed\")) #caressed\n",
    "print(lemmatizer.lemmatize(\"athlete\")) #athlete\n",
    "print(lemmatizer.lemmatize(\"athletics\")) #athletics\n",
    "print(lemmatizer.lemmatize(\"media\"))\n",
    "print(lemmatizer.lemmatize(\"photography\")) #photography\n",
    "print(lemmatizer.lemmatize(\"sexy\")) #sexy\n",
    "print(lemmatizer.lemmatize(\"journalling\")) #journalling\n",
    "print(lemmatizer.lemmatize(\"Slovakia\")) #Slovakia\n",
    "print(lemmatizer.lemmatize(\"corpora\")) # corpus\n",
    "print(lemmatizer.lemmatize(\"thieves\")) # thief\n",
    "print(lemmatizer.lemmatize(\"rocks\")) #rock\n",
    "```\n",
    "\n",
    "### Why Stemming?\n",
    "- smaller and faster\n",
    "- simplicity in \"good enough\"\n",
    "- can often **provide higher recall (coverage)** if you are using it for text searching: `drives` and `drivers` will likely shorten to `driv`, which may be useful if your search engine wants to make sure to get all relevant documents, even at the cost of surfacing a few irrelevant documents\n",
    "- could potentially be more useful for predictive models that tend to overfit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Differences Between Stemming and Lemmatization\n",
    "> **Stemming** usually refers to a crude heuristic process that chops off the ends of words in the hope of achieving this goal correctly most of the time, and often includes the removal of derivational affixes. **Lemmatization** usually refers to doing things properly with the use of a vocabulary and morphological analysis of words, normally aiming to remove inflectional endings only and to return the base or dictionary form of a word, which is known as the lemma . If confronted with the token `saw`, stemming might return just `s`, whereas lemmatization would attempt to return either `see` or `saw` depending on whether the use of the token was as a verb or a noun. ([Stanford NLP Group](https://nlp.stanford.edu/IR-book/html/htmledition/stemming-and-lemmatization-1.html)) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scoring Metrics\n",
    "\n",
    "<img src=\"images/confusion_matrix2.png\" alt=\"Different Stemming Techniques\" style=\"width:600px;\"/>\n",
    "\n",
    "### Precision/Recall\n",
    "\n",
    "**Recall:** What percent of the positive classes did the model successfully predict?\n",
    "**Precision:** When a model predicted a positive class, what percentage of the time was it correct?\n",
    "\n",
    "In terms of NLP / stemming / lemmatization:\n",
    "\n",
    "**Recall**: After processing (tokenizing, stemming/lemmatizing) the data, what percent of the relevant search results were surfaced? Ie. - when a user searches for `blue jeans`, did all the results returned include all the relevant items (blue-ish colored denim pants)?\n",
    "\n",
    "**Precision**: After processing (tokenizing, stemming/lemmatizing) the data, what percent of the results returned were relevant?\n",
    "\n",
    "<img src=\"images/matrix_practice3.png\" alt=\"Different Stemming Techniques\" style=\"width:600px;\"/>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### F1 Score\n",
    "\n",
    "The F1 score of a model represents the harmonic mean between precision and recall, and is defined as \n",
    "\n",
    "$$\n",
    "\\begin{equation}\n",
    "F_{1} = 2 * \\frac{P * R}{P + R}\n",
    "\\end{equation}\n",
    "$$\n",
    "\n",
    "An F1 score is often a good measure \n",
    "* our dataset target is class imbalanced (ie. 96% positive, 4% negative)\n",
    "* when we want to balance optimizing for both precision and recall"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "solution2": "hidden",
    "solution2_first": true
   },
   "source": [
    "### Exercises\n",
    "\n",
    "Calculate:\n",
    "\n",
    "1. Overall **accuracy**: $\\frac{?}{?}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "solution2": "hidden"
   },
   "source": [
    "If $C$ is the number of correct predictions, and $N$ is the number of total samples:\n",
    "\\begin{equation}\n",
    "a = \\frac{C}{N}\n",
    "\\end{equation}\n",
    "\n",
    "\\begin{equation}\n",
    "a = \\frac{30}{36}\n",
    "\\end{equation}\n",
    "\n",
    "\\begin{equation}\n",
    "a = 0.833\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "solution2": "hidden",
    "solution2_first": true
   },
   "source": [
    "2. **Precision:** $\\frac{?}{?}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "solution2": "hidden"
   },
   "source": [
    "If $TP$ is the number of true positives, and $FP$ is the number of false positives:\n",
    "\\begin{equation}\n",
    "P = \\frac{TP}{TP + FP}\n",
    "\\end{equation}\n",
    "\n",
    "\\begin{equation}\n",
    "P = \\frac{10}{10 + 2}\n",
    "\\end{equation}\n",
    "\n",
    "\\begin{equation}\n",
    "P = 0.833\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "solution2": "hidden",
    "solution2_first": true
   },
   "source": [
    "3. **Recall:** $\\frac{?}{?}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "solution2": "hidden"
   },
   "source": [
    "If $TP$ is the number of true positives, and $FN$ is the number of false negatives:\n",
    "\\begin{equation}\n",
    "R = \\frac{TP}{TP + FN}\n",
    "\\end{equation}\n",
    "\n",
    "\\begin{equation}\n",
    "R = \\frac{10}{10 + 4}\n",
    "\\end{equation}\n",
    "\n",
    "\\begin{equation}\n",
    "R = 0.714\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "solution2": "hidden",
    "solution2_first": true
   },
   "source": [
    "4. **F1 Score**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "solution2": "hidden"
   },
   "source": [
    "\\begin{equation}\n",
    "F_{1} = 2 * \\frac{P * R}{P + R}\n",
    "\\end{equation}\n",
    "\n",
    "\\begin{equation}\n",
    "F_{1} = 2 * \\frac{0.833 * 0.714}{0.833 + 0.714}\n",
    "\\end{equation}\n",
    "\n",
    "\\begin{equation}\n",
    "F_{1} =0.769\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Removing Stopwords\n",
    "\n",
    "It's your call ultimately if you want to remove stopwords. There are advantages and disadvantages to both approaches. You will first need to run `nltk.download(\"stopwords\")` to download the set of stopwords for NLTK:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'while', 'hers', 'shan', 'before', 'then', 'was', 'during', 'haven', 'doesn', 'mightn', \"should've\", 'each', 'having', 's', 'ma', 'a', 'out', 'has', 'yours', 'their', \"aren't\", 'needn', 'if', 'until', 'aren', 'down', 'when', 'off', 'or', 'on', 'more', 'd', 'shouldn', 'won', 'she', 'over', 're', 'ain', 'this', 'll', 'he', 'into', \"shan't\", 'don', 'hadn', 'for', 'where', 'both', \"that'll\", \"hasn't\", 'been', 'are', 'above', \"weren't\", \"needn't\", 'myself', 'weren', 'isn', 'o', 'just', 'them', 'only', 'few', 'some', 'in', 've', 'be', 'me', 'wasn', \"don't\", 'most', \"didn't\", 'now', \"you'd\", 'of', 'doing', 'couldn', 'we', 'it', 'her', 'didn', 'itself', 'yourselves', 'am', 'an', \"shouldn't\", 'being', 'to', \"wasn't\", 'you', 'themselves', 'himself', 'by', 'so', 'with', \"couldn't\", 'again', 'herself', 'other', 'no', 'through', \"you'll\", 'hasn', 'mustn', 'there', 'any', 'do', 'here', \"mightn't\", 't', 'same', 'theirs', \"doesn't\", 'not', \"you've\", 'how', \"she's\", 'up', 'our', 'm', 'further', 'nor', 'because', 'against', 'too', 'once', 'under', 'than', \"mustn't\", 'does', 'him', 'at', 'is', 'wouldn', 'will', 'why', \"it's\", 'after', 'what', \"won't\", \"isn't\", 'its', 'his', \"wouldn't\", 'ours', 'whom', 'and', 'yourself', 'i', 'had', 'did', 'own', \"you're\", 'that', 'those', 'between', 'such', 'below', 'they', 'all', 'but', 'y', \"haven't\", \"hadn't\", 'your', 'from', 'very', 'these', 'ourselves', 'can', 'were', 'should', 'which', 'have', 'who', 'the', 'about', 'my', 'as'}\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "print(set(stopwords.words('english'))) # see the set of words NLTK considers stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "# import in a dataset of baltimore's public art galleries\n",
    "public_art_df: pd.DataFrame = pd.read_csv(\"baltimore_public_art.csv\")\n",
    "\n",
    "public_art_df = public_art_df.replace(np.nan, '', regex=True)\n",
    "public_art_df.head()\n",
    "# use the titleOfArtwork field\n",
    "titles_of_artworks: pd.Series = public_art_df[\"titleOfArtwork\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CountVectorize the Documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "vectorizer = CountVectorizer()\n",
    "X = vectorizer.fit_transform(titles_of_artworks) \n",
    "X = X.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus_df = pd.DataFrame(X, columns=vectorizer.get_feature_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataframe shape was (690, 619)\n",
      "Dataframe shape is now(690, 597)\n"
     ]
    }
   ],
   "source": [
    "# iterate through the Pandas dataframe, and drop the columns that reflect stopwords:\n",
    "original_columns = corpus_df.columns # get existing columns\n",
    "\n",
    "to_drop_columns = set(original_columns).intersection(set(stopwords.words('english'))) # get the list of words to drop\n",
    "print(f\"Dataframe shape was {corpus_df.shape}\")\n",
    "corpus_df.drop(columns=to_drop_columns, inplace=True)\n",
    "print(f\"Dataframe shape is now{corpus_df.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### When Should You Avoid Removing Stopwords?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The below example is taken from a Medium article by Gagandeep Singh, [\"Why you should avoid removing STOPWORDS\"](https://towardsdatascience.com/why-you-should-avoid-removing-stopwords-aa7a353d2a52)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "reviews = [\n",
    "    (\"The product is really very good\", \"Positive\"),\n",
    "    (\"The products seems to be good\", \"Positive\"),\n",
    "    (\"Good product. I really liked it\", \"Positive\"),\n",
    "    (\"I didn’t like the product\", \"Negative\"),\n",
    "    (\"The product is not good.\", \"Negative\")\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned_reviews = []\n",
    "nltk_stopwords = set(stopwords.words('english'))\n",
    "\n",
    "# This iterates through each of the reviews, splitting the review into distinct tokens\n",
    "# Then it checks each token for whether or not it is a stopword, before adding them back into a \"cleaned_review\"\n",
    "for review in reviews:\n",
    "    words = review[0].split()\n",
    "    new_words = []\n",
    "    for word in words:\n",
    "        if word in nltk_stopwords:\n",
    "            continue\n",
    "        new_words.append(word)\n",
    "    cleaned_review = \" \".join(new_words)\n",
    "    cleaned_reviews.append((cleaned_review, review[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('The product really good', 'Positive'),\n",
       " ('The products seems good', 'Positive'),\n",
       " ('Good product. I really liked', 'Positive'),\n",
       " ('I didn’t like product', 'Negative'),\n",
       " ('The product good.', 'Negative')]"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cleaned_reviews"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Singh:\n",
    "> If you are working with **basic NLP techniques like BOW, Count Vectorizer or TF-IDF(Term Frequency and Inverse Document Frequency)** then removing stopwords is a good idea because stopwords act like noise for these methods. If you working with **LSTMs or other models which capture the semantic meaning and the meaning of a word depends on the context of the previous text**, then it becomes important not to remove stopwords."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise:\n",
    "\n",
    "##### 1. For each of the following statements, label them True or False. If False, briefly explain why:\n",
    "\n",
    "A. *Text typically should be processed via either stemming or lemmatization, but not both.*\n",
    "\n",
    "B. *Texts processed using lemmatization will typically have higher recall than stemming.*\n",
    "\n",
    "C. *If the **F1 score** of a model is **1.0 (100%)**, then the accuracy of your model must also be **100%**.*\n",
    "\n",
    "D. *Stemming **increases the size of the vocabulary** (the vocabulary is the set of all tokens found inside the corpus)*\n",
    "\n",
    "##### 2. Calculate precison and recall given the following results from a confusion matrix:\n",
    "\n",
    "<img src=\"images/exercise.jpeg\" alt=\"Different Stemming Techniques\" style=\"width:600px;\"/>\n",
    "\n",
    "##### 3. Provide an example of how stemming can improve recall.\n",
    "\n",
    "##### 4. Provide an example of when stemming might reduce precision."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Vectorization Techniques"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Count Vectorization\n",
    "<img src=\"images/count_vectorizer.png\" alt=\"Different Stemming Techniques\" style=\"width:600px;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can use **`sklearn.feature_extraction.text.CountVectorizer`** to easily convert your corpus into a bag of words matrix:\n",
    "\n",
    "```python\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "vectorizer = CountVectorizer()\n",
    "data_corpus = [\"John likes to watch movies. Mary likes movies too.\", \n",
    "\"John also likes to watch football games. Mary does not like football much.\"]\n",
    "X = vectorizer.fit_transform(data_corpus) \n",
    "```\n",
    "Note that the output `X` here is not your traditional Numpy matrix! Calling **`type(X)`** here will yield **`<class 'scipy.sparse.csr.csr_matrix'>`**, which is a **CSR ([compressed sparse row format matrix](https://docs.scipy.org/doc/scipy-0.14.0/reference/generated/scipy.sparse.csr_matrix.html))**. To convert it into an actual matrix, call the `toarray()` method:\n",
    "\n",
    "```python\n",
    "X.toarray()\n",
    "```\n",
    "Your output will be \n",
    "\n",
    "```\n",
    "array([[0, 0, 0, 0, 1, 0, 2, 1, 2, 0, 0, 1, 1, 1],\n",
    "       [1, 1, 2, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1]], dtype=int64)\n",
    "```\n",
    "Notice that using **`X.shape`** $\\rightarrow$ `(2,14)`, indicating a total vocabulary size $V$ of 14. To get what word each of the 14 columns corresponds to, use **`vectorizer.get_feature_names()`**:\n",
    "```\n",
    "['also', 'does', 'football', 'games', 'john', 'like', 'likes', 'mary', 'movies', 'much', 'not', 'to', 'too', 'watch']\n",
    "```\n",
    "\n",
    "Notice, however, that as the vocabulary size $V$ increases, the percent of the matrix taken up by zero values increases:\n",
    "\n",
    "```python\n",
    "corpus = [\n",
    "    \"Some analysts think demand could drop this year because a large number of homeowners take on remodeling projectsafter buying a new property. With fewer homes selling, home values easing, and mortgage rates rising, they predict home renovations could fall to their lowest levels in three years.\", \n",
    "    \n",
    "          \"Most home improvement stocks are expected to report fourth-quarter earnings next month.\",\n",
    "    \n",
    "         \"The conversation boils down to how much leverage management can get out of its wide-ranging efforts to re-energize operations, branding, digital capabilities, and the menu–and, for investors, how much to pay for that.\",\n",
    "    \n",
    "    \"RMD’s software acquisitions, efficiency, and mix overcame pricing and its gross margin improved by 90 bps Y/Y while its operating margin (including amortization) improved by 80 bps Y/Y. Since RMD expects the slower international flow generator growth to continue for the next few quarters, we have lowered our organic growth estimates to the mid-single digits.\"\n",
    "]\n",
    "\n",
    "X = vectorizer.fit_transform(corpus).toarray() \n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4, 59)\n",
      "<class 'scipy.sparse.csr.csr_matrix'>\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "# list of text documents\n",
    "text = [\"It's still early, so box-office disappointments are still among the highest-grossing movies of the year.\", \n",
    "        \"That movie was terrific\", \"You love cats\", \n",
    "        \"Pay for top executives at big US companies is vastly higher than what everyday workers make, and a new report from The Wall Street Journal has found that CEOs have hit an eye-popping milestone in the size of their monthly paychecks.\"]\n",
    "# create the transform\n",
    "vectorizer = CountVectorizer()\n",
    "\n",
    "# tokenize and build vocab\n",
    "vectorizer.fit(text)\n",
    "\n",
    "# vectorize the corpus\n",
    "vector = vectorizer.transform(text)\n",
    "\n",
    "# summarize encoded vector\n",
    "print(vector.shape)\n",
    "\n",
    "# Notice what type of object this is\n",
    "print(type(vector))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1 0 0 1 0 0 1 0 0 0 1 1 0 0 0 0 0 0 1 0 0 0 1 0 0 0 1 0 0 0 0 0 0 1 0 1\n",
      "  1 0 0 0 0 0 1 2 0 0 0 0 2 0 0 0 0 0 0 0 0 1 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0\n",
      "  0 0 0 0 0 0 0 0 0 1 0 1 0 0 0 0 0 0 1 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0\n",
      "  0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1]\n",
      " [0 1 1 0 1 1 0 0 1 1 0 0 1 1 1 1 1 1 0 1 1 1 0 1 1 1 0 1 0 1 1 1 0 0 1 1\n",
      "  0 1 1 1 1 1 0 0 1 0 1 1 2 1 1 1 1 1 0 1 1 0 0]]\n",
      "['among', 'an', 'and', 'are', 'at', 'big', 'box', 'cats', 'ceos', 'companies', 'disappointments', 'early', 'everyday', 'executives', 'eye', 'for', 'found', 'from', 'grossing', 'has', 'have', 'higher', 'highest', 'hit', 'in', 'is', 'it', 'journal', 'love', 'make', 'milestone', 'monthly', 'movie', 'movies', 'new', 'of', 'office', 'pay', 'paychecks', 'popping', 'report', 'size', 'so', 'still', 'street', 'terrific', 'than', 'that', 'the', 'their', 'top', 'us', 'vastly', 'wall', 'was', 'what', 'workers', 'year', 'you']\n"
     ]
    }
   ],
   "source": [
    "# see the outputted vectors\n",
    "print(vector.toarray())\n",
    "print(vectorizer.get_feature_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>among</th>\n",
       "      <th>an</th>\n",
       "      <th>and</th>\n",
       "      <th>are</th>\n",
       "      <th>at</th>\n",
       "      <th>big</th>\n",
       "      <th>box</th>\n",
       "      <th>cats</th>\n",
       "      <th>ceos</th>\n",
       "      <th>companies</th>\n",
       "      <th>...</th>\n",
       "      <th>their</th>\n",
       "      <th>top</th>\n",
       "      <th>us</th>\n",
       "      <th>vastly</th>\n",
       "      <th>wall</th>\n",
       "      <th>was</th>\n",
       "      <th>what</th>\n",
       "      <th>workers</th>\n",
       "      <th>year</th>\n",
       "      <th>you</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>4.00</td>\n",
       "      <td>4.00</td>\n",
       "      <td>4.00</td>\n",
       "      <td>4.00</td>\n",
       "      <td>4.00</td>\n",
       "      <td>4.00</td>\n",
       "      <td>4.00</td>\n",
       "      <td>4.00</td>\n",
       "      <td>4.00</td>\n",
       "      <td>4.00</td>\n",
       "      <td>...</td>\n",
       "      <td>4.00</td>\n",
       "      <td>4.00</td>\n",
       "      <td>4.00</td>\n",
       "      <td>4.00</td>\n",
       "      <td>4.00</td>\n",
       "      <td>4.00</td>\n",
       "      <td>4.00</td>\n",
       "      <td>4.00</td>\n",
       "      <td>4.00</td>\n",
       "      <td>4.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.25</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.25</td>\n",
       "      <td>...</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.50</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.50</td>\n",
       "      <td>...</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>...</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>...</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>...</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>0.25</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.25</td>\n",
       "      <td>...</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>1.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>...</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1.00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8 rows × 59 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       among    an   and   are    at   big   box  cats  ceos  companies  ...  \\\n",
       "count   4.00  4.00  4.00  4.00  4.00  4.00  4.00  4.00  4.00       4.00  ...   \n",
       "mean    0.25  0.25  0.25  0.25  0.25  0.25  0.25  0.25  0.25       0.25  ...   \n",
       "std     0.50  0.50  0.50  0.50  0.50  0.50  0.50  0.50  0.50       0.50  ...   \n",
       "min     0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00       0.00  ...   \n",
       "25%     0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00       0.00  ...   \n",
       "50%     0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00       0.00  ...   \n",
       "75%     0.25  0.25  0.25  0.25  0.25  0.25  0.25  0.25  0.25       0.25  ...   \n",
       "max     1.00  1.00  1.00  1.00  1.00  1.00  1.00  1.00  1.00       1.00  ...   \n",
       "\n",
       "       their   top    us  vastly  wall   was  what  workers  year   you  \n",
       "count   4.00  4.00  4.00    4.00  4.00  4.00  4.00     4.00  4.00  4.00  \n",
       "mean    0.25  0.25  0.25    0.25  0.25  0.25  0.25     0.25  0.25  0.25  \n",
       "std     0.50  0.50  0.50    0.50  0.50  0.50  0.50     0.50  0.50  0.50  \n",
       "min     0.00  0.00  0.00    0.00  0.00  0.00  0.00     0.00  0.00  0.00  \n",
       "25%     0.00  0.00  0.00    0.00  0.00  0.00  0.00     0.00  0.00  0.00  \n",
       "50%     0.00  0.00  0.00    0.00  0.00  0.00  0.00     0.00  0.00  0.00  \n",
       "75%     0.25  0.25  0.25    0.25  0.25  0.25  0.25     0.25  0.25  0.25  \n",
       "max     1.00  1.00  1.00    1.00  1.00  1.00  1.00     1.00  1.00  1.00  \n",
       "\n",
       "[8 rows x 59 columns]"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load vectorized corpus into Pandas dataframe\n",
    "import pandas as pd\n",
    "corpus_df = pd.DataFrame(vector.toarray(), columns=vectorizer.get_feature_names())\n",
    "corpus_df.describe()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "319px"
   },
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
