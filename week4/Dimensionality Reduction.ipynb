{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dimensionality Reduction\n",
    "\n",
    "## The Problem\n",
    "\n",
    "There is an interesting tradeoff between model performance and a feature's dimensionality (http://www.visiondummy.com/2014/04/curse-dimensionality-affect-classification/):\n",
    "![http://www.visiondummy.com/2014/04/curse-dimensionality-affect-classification/](https://raw.githubusercontent.com/ychennay/dso-560-nlp-text-analytics/main/images/dimensionality_vs_performance.png)\n",
    "\n",
    ">*If the amount of available training data is fixed, then overfitting occurs if we keep adding dimensions. On the other hand, if we keep adding dimensions, the amount of **training data needs to grow exponentially fast to maintain the same coverage** and to avoid overfitting* ([Computer Vision for Dummies](http://www.visiondummy.com/2014/04/curse-dimensionality-affect-classification/)).\n",
    "\n",
    "![http://www.visiondummy.com/2014/04/curse-dimensionality-affect-classification/](https://raw.githubusercontent.com/ychennay/dso-560-nlp-text-analytics/main/images/curseofdimensionality.png)\n",
    "\n",
    "### Multi-Collinearity\n",
    "\n",
    "In many cases, there is a high degree of correlation between many of the features in a dataset. For instance, suppose that you\n",
    "\n",
    "\n",
    "## Sparsity\n",
    "\n",
    "- High dimensionality increases the sparsity of your features (**what NLP techniques have we used that illustrate this point?**)\n",
    "- The density of the training samples decreases when dimensionality increases:\n",
    "- Distance measures (Euclidean, for instance) start losing their effectiveness, because there isn't much difference between the max and min distances in higher dimensions.\n",
    "- Many models that rely upon assumptions of Gaussian distributions (like OLS linear regression), Gaussian mixture models, Gaussian processes, etc. become less and less effective since their distributions become flatter and \"fatter tailed\".\n",
    "![http://www.visiondummy.com/2014/04/curse-dimensionality-affect-classification/](../images/distance-asymptote.png)\n",
    "\n",
    "What is the amount of data needed to maintain **20% coverage** of the feature space? For 1 dimension, it is **20% of the entire population's dataset**. For a dimensionality of $D$:\n",
    "\n",
    "$$\n",
    "X^{D} = .20\n",
    "$$\n",
    "$$\n",
    "(X^{D})^{\\frac{1}{D}} = .20^{\\frac{1}{D}}\n",
    "$$\n",
    "$$\n",
    "X = \\sqrt[D]{.20}\n",
    "$$\n",
    "You can approximate this as \n",
    "```python\n",
    "def coverage_requirement(requirement, D):\n",
    "    return requirement ** (1 / D)\n",
    "\n",
    "x = []\n",
    "y = []\n",
    "for d in range(1,20):\n",
    "    y.append(coverage_requirement(0.10, d))\n",
    "    x.append(d)\n",
    "    \n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.plot(x,y)\n",
    "plt.xlabel(\"Number of Dimensions\")\n",
    "plt.ylabel(\"Appromximate % of Population Dataset\")\n",
    "plt.title(\"% of Dataset Needed to Maintain 10% Coverage of Feature Space\")\n",
    "plt.show()\n",
    "```\n",
    "<img src=\"https://raw.githubusercontent.com/ychennay/dso-560-nlp-text-analytics/main/images/coverage-needed.png\" width=\"500\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "#reviews = pd.read_csv(\"mcdonalds-yelp-negative-reviews.csv\", encoding='latin-1')\n",
    "reviews = open(\"../datasets/mcdonalds-yelp-negative-reviews.csv\", encoding='latin-1')\n",
    "\n",
    "#text = reviews[\"review\"].values\n",
    "text = reviews.readlines()\n",
    "\n",
    "\n",
    "vectorizer = CountVectorizer(ngram_range=(3,3), min_df=0.01, max_df=0.75, max_features=200)\n",
    "# tokenize and build vocab\n",
    "vectorizer.fit(text)\n",
    "vector = vectorizer.transform(text)\n",
    "features = vector.toarray()\n",
    "features_df = pd.DataFrame(features, columns=vectorizer.get_feature_names())\n",
    "features_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "I love to eat McDonalds\n",
    "\n",
    "[I love to] [eat McDonalds END]\n",
    "[I love to] [love to eat] [to eat McDonalds]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "correlations = features_df.corr()\n",
    "correlations_stacked = correlations.stack().reset_index()\n",
    "#set column names\n",
    "correlations_stacked.columns = ['Tri-Gram 1','Tri-Gram 2','Correlation']\n",
    "correlations_stacked = correlations_stacked[correlations_stacked[\"Correlation\"] < 1]\n",
    "correlations_stacked = correlations_stacked.sort_values(by=['Correlation'], ascending=False)\n",
    "correlations_stacked.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install seaborn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "plt.rcParams[\"figure.figsize\"] = (15,15)\n",
    "sns.heatmap(correlations)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Principle Component Analysis\n",
    "\n",
    "If you have an original matrix $Z$, you can decompose this matrix into two smaller matrices $X$ and $Q$. \n",
    "\n",
    "## Important Points:\n",
    "\n",
    "- Multiplying a vector by a matrix typically changes the direction of the vector. For instance:\n",
    "<figure>\n",
    "  <img src=\"https://raw.githubusercontent.com/ychennay/dso-560-nlp-text-analytics/main/images/multvector.png\" alt=\"my alt text\"/>\n",
    "    <figcaption><a href=\"https://lazyprogrammer.me/tutorial-principal-components-analysis-pca\">Lazy Programmer- \n",
    "        Tutorial to PCA</a></figcaption>\n",
    "</figure>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However, there are eigenvalues Î» and eigenvectors $v$ such that\n",
    "\n",
    "$$\n",
    "\\sum{X}v = \\lambda v\n",
    "$$\n",
    "\n",
    "Multiplying the eigenvectors $v$ with the eigenvalue $\\lambda$ does not change the direction of the eigenvector.\n",
    "\n",
    "Multiplying the eigenvector $v$ by the covariance matrix $\\sum{X}$ also does not change the direction of the eigenvector.\n",
    "\n",
    "If our data $X$ is of shape $N \\times D$, it turns out that we have $D$ eigenvalues and $D$ eigenvectors. This means we can arrange the eigenvalues $\\lambda$ in decreasing order so that\n",
    "\n",
    "$$\n",
    "\\lambda_3 > \\lambda_2 > \\lambda_5\n",
    "$$\n",
    "\n",
    "In this case, $\\lambda_3$ is the largest eigenvalue, followed by $\\lambda_2$, and then $\\lambda_5$. Then, we can arrange \n",
    "\n",
    "We can also rearrange the eigenvectors the same: $v_3$ will be the first column, $v_2$ will be the second column, and $v_5$ will be the third column.\n",
    "\n",
    "We'll end up with two matrices $V$ and $\\Lambda$:\n",
    "<figure>\n",
    "  <img src=\"https://raw.githubusercontent.com/ychennay/dso-560-nlp-text-analytics/main/images/pca1.png\" alt=\"my alt text\"/>\n",
    "    <figcaption><a href=\"https://lazyprogrammer.me/tutorial-principal-components-analysis-pca\">Lazy Programmer- \n",
    "        Tutorial to PCA</a></figcaption>\n",
    "</figure>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# what is the shape of our features?\n",
    "features.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# features -> age, gpa, final_score, is_female, is_full_time\n",
    "\n",
    "a = [45, 3.4, 0, 1]\n",
    "b = [21, 3.5, 1, 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "\n",
    "pca = PCA(n_components=4)\n",
    "\n",
    "Z = pca.fit_transform(features)\n",
    "\n",
    "# what is the shape of Z?\n",
    "Z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# what will happen if we take the correlation matrix and covariance matrix of our new reduced features?\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "covariances = pd.DataFrame(np.cov(Z.transpose()))\n",
    "plt.rcParams[\"figure.figsize\"] = (5,5)\n",
    "sns.heatmap(covariances)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = PCA(n_components=2)\n",
    "Z_two_dimensions = pca.fit_transform(features)\n",
    "Z_two_dimensions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.scatter(Z_two_dimensions[:,0], Z_two_dimensions[:, 1])\n",
    "reduced_features_df = pd.DataFrame(Z_two_dimensions, columns=[\"x1\", \"x2\"])\n",
    "reduced_features_df[\"text\"] = text\n",
    "reduced_features_df.to_csv(\"reduced_features.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Singular Value Decomposition\n",
    "\n",
    "Given an input matrix $A$, we want to try to represent it instead as three smaller matrices $U$, $\\sum$, and $V$. Instead of **$n$ original terms**, we want to represent each document as **$r$ concepts** (other referred to as **latent dimensions**, or **latent factors**)- from [Mining of Massive Datasets - Dimensionality Reduction: Singular Value Decomposition](https://www.youtube.com/watch?v=P5mlg91as1c):\n",
    "<figure>\n",
    "  <img src=\"https://raw.githubusercontent.com/ychennay/dso-560-nlp-text-analytics/main/images/svd.png\" alt=\"my alt text\"/>\n",
    "    <figcaption><i>\n",
    "        <a href=\"https://www.youtube.com/watch?v=P5mlg91as1c\">Mining of Massive Datasets - Dimensionality Reduction: Singular Value Decomposition</a> by Leskovec, Rajaraman, and Ullman (Stanford University)</i></figcaption>\n",
    "</figure>\n",
    "\n",
    "Here, **$A$ is your matrix of word vectors** - you could use any of the word vectorization techniques we have learned so far, include one-hot encoding, word count, TF-IDF.\n",
    "\n",
    "- $\\sum$ will be a **diagonal matrix** with values that are positive and sorted in decreasing order. Its value indicate the **variance (information encoded on that new dimension)**- therefore, the higher the value, the stronger that dimension is in capturing data from A, the original features. For our purposes, we can think of the rank of this $\\sum$ matrix as the number of desired dimensions. Instance, if we want to reduce $A$ from shape $1020 x 300$ to $1020 x 10$, we will want to reduce the rank of $\\sum$ from 300 to 10.\n",
    "\n",
    "- $U^T U = I$ and $V^T V = I$\n",
    "\n",
    "## Measuring the Quality of the Reconstruction\n",
    "\n",
    "A popular metric used for measuring the quality of the reconstruction is the [Frobenius Norm](https://en.wikipedia.org/wiki/Matrix_norm#Frobenius_norm). When you explain your methodology for reducing dimensions, usually managers / stakeholders will want to some way to compare multiple dimensionality techniques' ability to quantify its ability to retain information but trim dimensions:\n",
    "\n",
    "$$\n",
    "\\begin{equation}\n",
    "||A_{old}-A_{new}||_{F} = \\sqrt{\\sum_{ij}{(A^{old}_{ij}- A^{new}_{ij}}})^2\n",
    "\\end{equation}\n",
    "$$\n",
    "\n",
    "## Heuristic Step for How Many Dimensions to Keep\n",
    "\n",
    "1. Sum the $\\sum$ matrix's diagonal values: \n",
    "$$\n",
    "\\begin{equation}\n",
    "\\sum_{i}^{m}\\sigma_{i}\n",
    "\\end{equation}\n",
    "$$\n",
    "\n",
    "2. Define your threshold of \"information\" (variance) $\\alpha$ to keep: usually 80% to 90%. \n",
    "\n",
    "3. Define your cutoff point $C$: $$\n",
    "\\begin{equation}\n",
    "C = \\sum_{i}^{m}\\sigma_{i} \\alpha\n",
    "\\end{equation}\n",
    "$$\n",
    "\n",
    "4. Beginning with your largest singular value, sum your singular values $\\sigma_{i}$ until it is greater than C. Retain only those dimensions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.linalg import svd\n",
    "x = np.linspace(1,20, 20) # create the first dimension\n",
    "x = np.concatenate((x,x))\n",
    "y = x + np.random.normal(0,1, 40) # create the second dimension\n",
    "z = x + np.random.normal(0,2, 40) # create the third dimension\n",
    "a = x + np.random.normal(0,4, 40) # create the fourth dimension\n",
    "plt.scatter(x,y) # plot just the first two dimensions\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "A = np.stack([x,y,z,a]).T\n",
    "A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "D = 1\n",
    "U, s, V = svd(A)\n",
    "print(f\"s is {s} ({s.shape})\\n\")\n",
    "print(f\"U is {U} ({U.shape})\\n\")\n",
    "print(f\"V is {V} ({V.shape})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s[D:] = 0\n",
    "S = np.zeros((A.shape[0], A.shape[1]))\n",
    "S[:A.shape[1], :A.shape[1]] = np.diag(s)\n",
    "A_reconstructed = U.dot(S.dot(V))\n",
    "np.sum((A_reconstructed - A) ** 2) ** (1/2) # Frobenius norm\n",
    "# reconstruct matrix\n",
    "U.dot(S)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Toy Example of SVD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define a matrix\n",
    "B = np.array([[1, 2], [3, 4], [5, 6]])\n",
    "print(\"B:\\n\", B)\n",
    "# Singular-value decomposition\n",
    "U, s, VT = svd(B)\n",
    "# create m x n Sigma matrix\n",
    "Sigma = np.zeros((B.shape[0],B.shape[1]))\n",
    "# populate Sigma with n x n diagonal matrix\n",
    "Sigma[:B.shape[1], :B.shape[1]] = np.diag(s)\n",
    "Sigma"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scikit Learn Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import TruncatedSVD\n",
    "\n",
    "svd = TruncatedSVD(n_components=5)\n",
    "reduced_svd_features = svd.fit_transform(features)\n",
    "reduced_svd_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "svd.singular_values_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "svd.explained_variance_.sum()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
