{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "graphic-noise",
   "metadata": {},
   "source": [
    "# Homework 2 Feedback"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "massive-beach",
   "metadata": {},
   "source": [
    "### Use a Word Count to Check for Additional Stopwords\n",
    "\n",
    "Students would first perform a word count to check for the top frequency words - then, they'd use their domain knowledge to select stopwords to add to the list. For instance, if you look carefully, many of the reviews contain `<br>` or `<br/>`. This is an HTML tag for a line break. This is definitely a word that should just be completely removed - you will catch this if you remove all non alphanumeric characters, tokenize, and perform a word count: `br` shows up as one of the top reoccuring words.\n",
    "\n",
    "### Expect to See Num Features with Stemming Should Be Generally Less Than Num Features with Lemmatization\n",
    "\n",
    "In the second part of the homework, we asked you to count the number of features you see with different text preprocessing techniques. You should make sure that you see less features after lemmatizing vs. after stemming. \n",
    "\n",
    "Some students submitted notebooks where the number of features for lemmatization was greater than with stemming, or the number of features with lemmatization + stopwords was greater than with just lemmatization. Both indicate some bug or issue in the code.\n",
    "\n",
    "### Don't Set Too High A Minimum DF Threshold for `CountVectorizer`\n",
    "\n",
    "For the first part of HW2, we saw many instances of `CountVectorizer` being initialized like this:\n",
    "```\n",
    "vectorizer = CountVectorizer(min_df=0.03)\n",
    "```\n",
    "Here are some issues with this:\n",
    "* `min_df=0.03` is actually quite high a bar. This means that the word needs to appear in 3% of all reviews. Given that the entire dataset (good + poor reviews) is 115k, this means the word needs to appear in 3,450 reviews or more to make it as a final feature. This actually means there's a very good chance it's a stopword! Especially since the product reviews are frequently about a wide variety of different toy products, you should inspect the final features you get and do a sanity-check to make sure they seem to be useful. You can also set a `max_df=UPPER_THRESHOLD` to limit stopwords that appear in too many documents.\n",
    "* we should be ideally using `binary=True` to offsite the possibility someone just writes the same word over and over again in the same review\n",
    "\n",
    "### Save Yourself Work By Using `CountVectorizer`'s Tokenizer\n",
    "\n",
    "Many students wrote their own functions to remove punctuation, or digits, and only keep alphabetical characters `A-Z`.\n",
    "\n",
    "However, you could also do all of that, plus remove stopwords all in one line:\n",
    "```python\n",
    "vectorizer = CountVectorizer(stopwords=my_custom_stopwords, \n",
    "                             token_pattern='\\b[a-zA-Z]{2,}\\b', \n",
    "                             min_df=0.01)\n",
    "```\n",
    "\n",
    "\n",
    "### Sanity Check Your Final Features\n",
    "\n",
    "Some students' final features list looked liked this:\n",
    "```\n",
    "'aa'  'aaa'  'aaaa' ...\n",
    "```\n",
    "These should ultimately be grouped together using regex, since they are basically the same exact word. Others had\n",
    "```\n",
    "034    077    099     9   91 ...\n",
    "```\n",
    "Check how `034` and `077` are being used in the reviews themselves. Often, it's used as a unit of currency or time. If that is the case, you ideally should write a regex pattern to group together all tokens that represent money or time (ie., group them as `_MONEY_` and `_TIME_`).\n",
    "\n",
    "Some students did this by adding phrases to the stopword set:\n",
    "```python\n",
    "# Appending stop words that appeared in final corpus that did not contain meaning.\n",
    "stop.append('10')\n",
    "stop.append('34')\n",
    "stop.append('wa')\n",
    "stop.append('br')\n",
    "stop.append('ca')\n",
    "stop.append('ha')\n",
    "```\n",
    "\n",
    "### Don't Repeat Yourself In Regex\n",
    "\n",
    "People wrote regex patterns to group together words they found that expressed similar meanings. For example,\n",
    "\n",
    "```python\n",
    "christmas_pattern = r'\\b(christ(?:-?)mas)\\b|\\b((?:x|x{3})(?:\\s|-)mas)\\b'\n",
    "```\n",
    "However, there's several ways to improve this:\n",
    "* no need to write `\\b` multiple times\n",
    "* combine the group for `christ` and `x`\n",
    "\n",
    "A shortened version that works just as well is:\n",
    "```python\n",
    "christmas_pattern = r'\\b(?:christ|(?:x|x{3}))[-\\s]?mas\\b'\n",
    "```\n",
    "See [RegExr sandbox](regexr.com/6isuh).\n",
    "\n",
    "### Nitpick: Use Type Hints and Docstrings to Improve Your Code's Readability\n",
    "\n",
    "* [Type Hints in Python](https://towardsdatascience.com/type-hints-in-python-everything-you-need-to-know-in-5-minutes-24e0bad06d0b)\n",
    "* [Docstrings](https://www.programiz.com/python-programming/docstrings)\n",
    "\n",
    "### Nitpick: Use Snake Case in Python\n",
    "\n",
    "Sometimes students will write variables as `pandasDf` or functions as `def countVectorize(...):`. This is not wrong, but in general, Python will use [snake case](https://en.wikipedia.org/wiki/Snake_case) instead of [camel case](https://en.wikipedia.org/wiki/Camel_case), which is more prevalent in most other programming languages like Java or JavaScript."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "constitutional-porter",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
