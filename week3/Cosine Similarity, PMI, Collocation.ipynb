{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tuning Count Vectorization - One Hot Encoding and other Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of dataframe is (655, 200)\n",
      "Total number of occurences: 29652\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>able</th>\n",
       "      <th>accidentally</th>\n",
       "      <th>agrees</th>\n",
       "      <th>appears</th>\n",
       "      <th>arrive</th>\n",
       "      <th>arrives</th>\n",
       "      <th>asks</th>\n",
       "      <th>attack</th>\n",
       "      <th>attacks</th>\n",
       "      <th>attempt</th>\n",
       "      <th>...</th>\n",
       "      <th>way</th>\n",
       "      <th>wife</th>\n",
       "      <th>woman</th>\n",
       "      <th>work</th>\n",
       "      <th>working</th>\n",
       "      <th>world</th>\n",
       "      <th>year</th>\n",
       "      <th>years</th>\n",
       "      <th>york</th>\n",
       "      <th>young</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 200 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   able  accidentally  agrees  appears  arrive  arrives  asks  attack  \\\n",
       "0     0             0       0        1       0        0     1       1   \n",
       "1     0             0       0        0       0        0     0       0   \n",
       "2     0             0       1        0       0        0     0       0   \n",
       "3     0             0       0        0       0        1     0       0   \n",
       "4     0             0       0        1       1        0     1       0   \n",
       "\n",
       "   attacks  attempt  ...  way  wife  woman  work  working  world  year  years  \\\n",
       "0        1        0  ...    0     0      1     0        0      0     0      0   \n",
       "1        1        0  ...    1     0      1     0        0      1     0      0   \n",
       "2        0        0  ...    0     1      0     0        0      0     0      0   \n",
       "3        0        0  ...    0     1      0     0        0      0     1      1   \n",
       "4        0        0  ...    1     1      0     0        0      0     0      0   \n",
       "\n",
       "   york  young  \n",
       "0     0      1  \n",
       "1     0      0  \n",
       "2     0      0  \n",
       "3     0      0  \n",
       "4     0      1  \n",
       "\n",
       "[5 rows x 200 columns]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "plots_df = pd.read_csv(\"movie_plots.csv\")\n",
    "\n",
    "# filter only for American movies\n",
    "plots_df = plots_df[plots_df[\"Origin/Ethnicity\"] == \"American\"]\n",
    "\n",
    " # traditional CountVectorizer\n",
    "vectorizer = CountVectorizer()\n",
    "\n",
    " # use English stopwords, and use one-hot encoding\n",
    "vectorizer = CountVectorizer(stop_words=\"english\", binary=True)\n",
    "\n",
    "# use English stopwords, and use one-hot encoding, and the word must appear in at least two of the movie plots\n",
    "vectorizer = CountVectorizer(stop_words=\"english\", binary=True, min_df=0.05) \n",
    "\n",
    "# use English stopwords, and use one-hot encoding, and the word must appear in at least two of the movie plots\n",
    "# and keep only the top 200\n",
    "vectorizer = CountVectorizer(stop_words=\"english\", binary=True, min_df=2, max_features=200) \n",
    "\n",
    "# use English stopwords, and use one-hot encoding, and the word must appear in at least two of the movie plots\n",
    "# and keep only the top 200\n",
    "vectorizer = CountVectorizer(stop_words=\"english\", binary=True, min_df=2, max_features=200) \n",
    "\n",
    "X = vectorizer.fit_transform(plots_df[\"Plot\"])\n",
    "\n",
    "vectorized_df = pd.DataFrame(X.toarray(), columns=vectorizer.get_feature_names())\n",
    "print(f\"Shape of dataframe is {vectorized_df.shape}\")\n",
    "print(f\"Total number of occurences: {vectorized_df.sum().sum()}\")\n",
    "#print(f\"Word counts: {vectorized_df.sum()}\")\n",
    "vectorized_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cosine Similarity Example\n",
    "\n",
    "### Intro to Algorithmic Marketing:\n",
    "![alt text](images/cos-sim-textbook1.png \"Logo Title Text 1\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Finding Magnitude of a Vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First approach: 3.7416573867739413\n",
      "Second approach: 3.7416573867739413\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "import numpy as np\n",
    "def magnitude(x): \n",
    "    return math.sqrt(sum(i**2 for i in x))\n",
    "\n",
    "vectorA = [0,3,1,2]\n",
    "\n",
    "print(f\"First approach: {magnitude(vectorA)}\")\n",
    "print(f\"Second approach: {np.linalg.norm(vectorA)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pointwise Mutual Information\n",
    "\n",
    "It's important to identify a **context window** when analyzing co-occurence. In the image below, the context window size is 4 (2 tokens to either side of the target word):\n",
    "\n",
    "![alt text](images/context_window.png \"Logo Title Text 1\")\n",
    "\n",
    "For the purposes of the next section, we'll define the **entire document as the context window.**\n",
    "\n",
    "Pointwise mutual information measures the ratio between the **joint probability of two events happening** with the probabilities of the two events happening, assuming they are independent. It can be defined with the following equation:\n",
    "\n",
    "$$\n",
    "PMI_{A,B} = log\\frac{p(A,B)}{p(A)p(B)}\n",
    "$$\n",
    "\n",
    "Remember that when two events are independent, $P(i,j) = P(i)P(j)$. Using PMI to just a raw word count is often preferable because very common words have extreme skew (\"the\" and \"of\" will co-occur frequently in the same  )\n",
    "\n",
    "```python\n",
    "import math\n",
    "def pmi(tokenA, tokenB, documents, word_counts):\n",
    "    \n",
    "    # word_counts[token_A] => number of times tokenA appears in the documents\n",
    "    # float(len(documents)) => number of documents\n",
    "    # bigram_freq => a dictionary of the number of times tokenA and tokenB are in the same document together\n",
    "    \n",
    "    prob_A = word_counts[tokenA] / float(len(documents))\n",
    "    prob_B = word_counts[tokenB] / float(len(documents))\n",
    "    prob_A_B = bigram_freq[\" \".join([tokenA, tokenB])] / float(len(documents))\n",
    "    return math.log(prob_A_B/float(prob_A*prob_B),2) \n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Collocation\n",
    "\n",
    "Many times, in previous homeworks, we've had to manually try to find phrases that belong together. For example, `New York City`.\n",
    "\n",
    "From [nltk.org](http://www.nltk.org/howto/collocations.html), **collocation** can be defined as\n",
    "\n",
    "> expressions of multiple words which commonly co-occur together. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.collocations import BigramCollocationFinder, BigramAssocMeasures\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk import word_tokenize\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "from nltk.corpus import stopwords\n",
    "stopwords = set(stopwords.words('english') + [\".\",'.', \",\",\":\", \"''\", \"'s\", \"'\", \"``\", \"(\", \")\", \"-\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "documents = []\n",
    "articles = [f\"bbcsport/football/00{i}.txt\" for i in range(1,10)]\n",
    "\n",
    "for article in articles:\n",
    "    article = open(article) # open each sports article\n",
    "    for line in article.readlines():\n",
    "        line = line.replace(\"\\n\", \"\") # replace the new line escape character\n",
    "        if len(line) > 0: # if the line is not empty, process it\n",
    "            line = [lemmatizer.lemmatize(token) for token in word_tokenize(line)] \n",
    "            documents.append(line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_documents = []\n",
    "for doc in documents:\n",
    "    new_document = []\n",
    "    for word in doc:\n",
    "        if word.strip().lower() not in stopwords:\n",
    "            new_document.append(word)\n",
    "    new_documents.append(new_document)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Champions', 'League'),\n",
       " ('Manchester', 'United'),\n",
       " ('Cristiano', 'Ronaldo'),\n",
       " ('Van', 'Nistelrooy'),\n",
       " ('Wayne', 'Rooney'),\n",
       " ('Alex', 'Ferguson'),\n",
       " ('FA', 'Cup'),\n",
       " ('Ferguson', 'wa'),\n",
       " ('Gary', 'Neville'),\n",
       " ('Man', 'Utd'),\n",
       " ('Manchester', 'City'),\n",
       " ('Sir', 'Alex'),\n",
       " ('national', 'team'),\n",
       " ('wa', \"n't\"),\n",
       " ('23', 'minute')]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "collocation_finder = BigramCollocationFinder.from_documents(new_documents)\n",
    "measures = BigramAssocMeasures()\n",
    "\n",
    "collocation_finder.nbest(measures.raw_freq, 15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Term Frequency / Inverse Document Frequency\n",
    "\n",
    "\n",
    "## Term Frequency\n",
    "![alt text](images/tf-idf1.png \"Term Frequency\")\n",
    "\n",
    "## Inverse Document Frequency\n",
    "![alt text](images/tf-idf2.png \"Inverse Document Frequency\")\n",
    "\n",
    "### Example Calculation\n",
    "\n",
    "![alt text](images/tf-idf4.png \"Example\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using Scikit-Learn to Generate TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "vectorizer = TfidfVectorizer(ngram_range=(3,4),\n",
    "                             token_pattern=r'\\b[a-zA-Z]{3,}\\b',\n",
    "                             max_df=0.4, stop_words=stopwords.words())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/site-packages/sklearn/feature_extraction/text.py:300: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['baiknya', 'berkali', 'kali', 'kurangnya', 'mata', 'olah', 'printr', 'sekurang', 'setidak', 'tama', 'tidaknya'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv(\"mcdonalds-yelp-negative-reviews.csv\", encoding=\"latin1\")\n",
    "corpus = list(df[\"review\"].values)\n",
    "\n",
    "X = vectorizer.fit_transform(corpus)\n",
    "terms = vectorizer.get_feature_names()\n",
    "tf_idf = pd.DataFrame(X.toarray().transpose(), index=terms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf_idf = tf_idf.sum(axis=1)\n",
    "score = pd.DataFrame(tf_idf, columns=[\"score\"])\n",
    "score.sort_values(by=\"score\", ascending=False, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "score.to_csv(\"scores.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercises\n",
    "\n",
    "For the following exercises, use the definitions below:\n",
    "\n",
    "**Term frequency**:\n",
    "$$\n",
    "tf = n(t,d)\n",
    "$$\n",
    "**Inverse document frequency**:\n",
    "$$\n",
    "idf = 1 + \\frac{N}{df(t) + 1}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "documents = [\n",
    "    \"He ate the food\",\n",
    "    \"He liked the meal\",\n",
    "    \"She likes the food from McDonalds, but she avoids the food from Burger King\",\n",
    "    \"They like to eat 3 meals a day\"\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculate the TF-IDF score for `like` in each of the documents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculate the TF-IDF score for `the food` bigram in each of the documents"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
