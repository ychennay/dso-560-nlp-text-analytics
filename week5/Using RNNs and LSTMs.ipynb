{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#Using-RNNs-and-LSTMs-in-Keras-for-NLP-Classification-Tasks\" data-toc-modified-id=\"Using-RNNs-and-LSTMs-in-Keras-for-NLP-Classification-Tasks-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>Using RNNs and LSTMs in Keras for NLP Classification Tasks</a></span><ul class=\"toc-item\"><li><span><a href=\"#Problem-Statement:-Predict-Sentiment-of-Amazon-Reviews\" data-toc-modified-id=\"Problem-Statement:-Predict-Sentiment-of-Amazon-Reviews-1.1\"><span class=\"toc-item-num\">1.1&nbsp;&nbsp;</span>Problem Statement: Predict Sentiment of Amazon Reviews</a></span></li><li><span><a href=\"#Tokenize-Text\" data-toc-modified-id=\"Tokenize-Text-1.2\"><span class=\"toc-item-num\">1.2&nbsp;&nbsp;</span>Tokenize Text</a></span></li><li><span><a href=\"#Import-Keras-Toolkit\" data-toc-modified-id=\"Import-Keras-Toolkit-1.3\"><span class=\"toc-item-num\">1.3&nbsp;&nbsp;</span>Import Keras Toolkit</a></span></li><li><span><a href=\"#Load-in-GloVe-Vectors\" data-toc-modified-id=\"Load-in-GloVe-Vectors-1.4\"><span class=\"toc-item-num\">1.4&nbsp;&nbsp;</span>Load in GloVe Vectors</a></span></li><li><span><a href=\"#Load-in-Embeddings\" data-toc-modified-id=\"Load-in-Embeddings-1.5\"><span class=\"toc-item-num\">1.5&nbsp;&nbsp;</span>Load in Embeddings</a></span></li><li><span><a href=\"#Define-in-Model\" data-toc-modified-id=\"Define-in-Model-1.6\"><span class=\"toc-item-num\">1.6&nbsp;&nbsp;</span>Define in Model</a></span></li><li><span><a href=\"#Compile-Model\" data-toc-modified-id=\"Compile-Model-1.7\"><span class=\"toc-item-num\">1.7&nbsp;&nbsp;</span>Compile Model</a></span><ul class=\"toc-item\"><li><ul class=\"toc-item\"><li><span><a href=\"#Helpful-Rule-of-Thumb-for-Defining-#-of-Parameters-in-LSTM:\" data-toc-modified-id=\"Helpful-Rule-of-Thumb-for-Defining-#-of-Parameters-in-LSTM:-1.7.0.1\"><span class=\"toc-item-num\">1.7.0.1&nbsp;&nbsp;</span>Helpful Rule of Thumb for Defining # of Parameters in LSTM:</a></span></li></ul></li></ul></li><li><span><a href=\"#Fit-the-Model\" data-toc-modified-id=\"Fit-the-Model-1.8\"><span class=\"toc-item-num\">1.8&nbsp;&nbsp;</span>Fit the Model</a></span></li><li><span><a href=\"#Evaluate-the-Model\" data-toc-modified-id=\"Evaluate-the-Model-1.9\"><span class=\"toc-item-num\">1.9&nbsp;&nbsp;</span>Evaluate the Model</a></span></li><li><span><a href=\"#Try-Some-Random-Reviews\" data-toc-modified-id=\"Try-Some-Random-Reviews-1.10\"><span class=\"toc-item-num\">1.10&nbsp;&nbsp;</span>Try Some Random Reviews</a></span></li></ul></li><li><span><a href=\"#Problem-Statement:-Predict-City-of-McDonalds-Review\" data-toc-modified-id=\"Problem-Statement:-Predict-City-of-McDonalds-Review-2\"><span class=\"toc-item-num\">2&nbsp;&nbsp;</span>Problem Statement: Predict City of McDonalds Review</a></span><ul class=\"toc-item\"><li><ul class=\"toc-item\"><li><span><a href=\"#Load-in-McDonalds-Yelp-Reviews\" data-toc-modified-id=\"Load-in-McDonalds-Yelp-Reviews-2.0.1\"><span class=\"toc-item-num\">2.0.1&nbsp;&nbsp;</span>Load in McDonalds Yelp Reviews</a></span></li></ul></li><li><span><a href=\"#Get-Your-Feature-Space-and-Target-Labels\" data-toc-modified-id=\"Get-Your-Feature-Space-and-Target-Labels-2.1\"><span class=\"toc-item-num\">2.1&nbsp;&nbsp;</span>Get Your Feature Space and Target Labels</a></span></li><li><span><a href=\"#Perform-Label-Categorical-Encoding-of-Cities\" data-toc-modified-id=\"Perform-Label-Categorical-Encoding-of-Cities-2.2\"><span class=\"toc-item-num\">2.2&nbsp;&nbsp;</span>Perform Label Categorical Encoding of Cities</a></span></li><li><span><a href=\"#Remove-Stopwords-Using-SpaCy\" data-toc-modified-id=\"Remove-Stopwords-Using-SpaCy-2.3\"><span class=\"toc-item-num\">2.3&nbsp;&nbsp;</span>Remove Stopwords Using SpaCy</a></span></li><li><span><a href=\"#Tokenize-the-Text\" data-toc-modified-id=\"Tokenize-the-Text-2.4\"><span class=\"toc-item-num\">2.4&nbsp;&nbsp;</span>Tokenize the Text</a></span></li><li><span><a href=\"#Integer-Encode-Tokens\" data-toc-modified-id=\"Integer-Encode-Tokens-2.5\"><span class=\"toc-item-num\">2.5&nbsp;&nbsp;</span>Integer Encode Tokens</a></span></li><li><span><a href=\"#Get-Max-Length-Per-Token\" data-toc-modified-id=\"Get-Max-Length-Per-Token-2.6\"><span class=\"toc-item-num\">2.6&nbsp;&nbsp;</span>Get Max Length Per Token</a></span></li><li><span><a href=\"#Split-into-Train/Test-Split\" data-toc-modified-id=\"Split-into-Train/Test-Split-2.7\"><span class=\"toc-item-num\">2.7&nbsp;&nbsp;</span>Split into Train/Test Split</a></span></li><li><span><a href=\"#Keras-RNN-Architecture\" data-toc-modified-id=\"Keras-RNN-Architecture-2.8\"><span class=\"toc-item-num\">2.8&nbsp;&nbsp;</span>Keras RNN Architecture</a></span></li><li><span><a href=\"#Load-in-Glove-Vectors\" data-toc-modified-id=\"Load-in-Glove-Vectors-2.9\"><span class=\"toc-item-num\">2.9&nbsp;&nbsp;</span>Load in Glove Vectors</a></span></li><li><span><a href=\"#Load-in-the-Embeddings\" data-toc-modified-id=\"Load-in-the-Embeddings-2.10\"><span class=\"toc-item-num\">2.10&nbsp;&nbsp;</span>Load in the Embeddings</a></span></li><li><span><a href=\"#Define-Model\" data-toc-modified-id=\"Define-Model-2.11\"><span class=\"toc-item-num\">2.11&nbsp;&nbsp;</span>Define Model</a></span></li><li><span><a href=\"#Compile-the-Model\" data-toc-modified-id=\"Compile-the-Model-2.12\"><span class=\"toc-item-num\">2.12&nbsp;&nbsp;</span>Compile the Model</a></span></li><li><span><a href=\"#Fit-the-Model\" data-toc-modified-id=\"Fit-the-Model-2.13\"><span class=\"toc-item-num\">2.13&nbsp;&nbsp;</span>Fit the Model</a></span></li><li><span><a href=\"#Evaluate-the-Model\" data-toc-modified-id=\"Evaluate-the-Model-2.14\"><span class=\"toc-item-num\">2.14&nbsp;&nbsp;</span>Evaluate the Model</a></span></li></ul></li></ul></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using RNNs and LSTMs in Keras for NLP Classification Tasks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem Statement: Predict Sentiment of Amazon Reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "NUM_SAMPLES = 3000\n",
    "\n",
    "good_reviews = open(\"../week1/good_amazon_toy_reviews.txt\").readlines()\n",
    "bad_reviews = open(\"../week1/poor_amazon_toy_reviews.txt\").readlines()\n",
    "\n",
    "sampled_good_reviews = good_reviews[:NUM_SAMPLES]\n",
    "sampled_bad_reviews = bad_reviews[:NUM_SAMPLES]\n",
    "\n",
    "docs = sampled_good_reviews + sampled_bad_reviews\n",
    "labels = np.concatenate([np.ones(NUM_SAMPLES), np.zeros(NUM_SAMPLES)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import en_core_web_md\n",
    "import spacy\n",
    "from scipy.spatial.distance import cosine\n",
    "nlp = en_core_web_md.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "stopwords_removed_docs = list(\n",
    "    map(lambda doc: \" \".join([token.text for token in nlp(doc) if not token.is_stop]), docs))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenize Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing.text import Tokenizer\n",
    "tokenizer = Tokenizer(num_words=5000, oov_token=\"UNKNOWN_TOKEN\")\n",
    "tokenizer.fit_on_texts(stopwords_removed_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def integer_encode_documents(docs, tokenizer):\n",
    "    return tokenizer.texts_to_sequences(docs)\n",
    "\n",
    "# integer encode the documents\n",
    "encoded_docs = integer_encode_documents(stopwords_removed_docs, tokenizer)\n",
    "# this is a list of lists, the numbers represent the index position of that word.\n",
    "# for instance, 33 means the 33rd word in the vocabulary\n",
    "# Notice the last document has 4 numbers, since it is a 4 word document: Could have done better.\n",
    "padded_docs = pad_sequences(encoded_docs, maxlen=MAX_SEQUENCE_LENGTH, padding='post')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_SEQUENCE_LENGTH = 100\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "encoder = LabelEncoder()\n",
    "labels = to_categorical(encoder.fit_transform(labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(padded_docs, labels, test_size=0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Keras Toolkit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from random import randint\n",
    "from numpy import array, argmax, asarray, zeros\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "VOCAB_SIZE = int(len(tokenizer.word_index) * 1.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load in GloVe Vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_glove_vectors():\n",
    "    embeddings_index = {}\n",
    "    with open('glove.6B.100d.txt') as f:\n",
    "        for line in f:\n",
    "            values = line.split()\n",
    "            word = values[0]\n",
    "            coefs = asarray(values[1:], dtype='float32')\n",
    "            embeddings_index[word] = coefs\n",
    "    print('Loaded %s word vectors.' % len(embeddings_index))\n",
    "    return embeddings_index\n",
    "\n",
    "\n",
    "embeddings_index = load_glove_vectors()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load in Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a weight matrix for words in training docs\n",
    "embedding_matrix = zeros((VOCAB_SIZE, 100))\n",
    "for word, i in tokenizer.word_index.items():\n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    if embedding_vector is not None: # check that it is an actual word that we have embeddings for\n",
    "        embedding_matrix[i] = embedding_vector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define in Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers.recurrent import SimpleRNN, LSTM\n",
    "from keras.layers import Flatten, Masking\n",
    "# define model\n",
    "\n",
    "def make_binary_classification_rnn_model(plot=False):\n",
    "    model = Sequential()\n",
    "    model.add(Embedding(VOCAB_SIZE, 100, weights=[embedding_matrix], input_length=MAX_SEQUENCE_LENGTH, trainable=False))\n",
    "    model.add(Masking(mask_value=0.0)) # masking layer, masks any words that don't have an embedding as 0s.\n",
    "    model.add(SimpleRNN(units=64, input_shape=(1, MAX_SEQUENCE_LENGTH)))\n",
    "    model.add(Dense(16))\n",
    "    model.add(Dense(2, activation='softmax'))\n",
    "    \n",
    "    # Compile the model\n",
    "    model.compile(\n",
    "    optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "    # summarize the model\n",
    "    model.summary()\n",
    "    \n",
    "    if plot:\n",
    "        plot_model(model, to_file='model.png', show_shapes=True)\n",
    "    return model\n",
    "\n",
    "def make_lstm_classification_model(plot=False):\n",
    "    model = Sequential()\n",
    "    model.add(Embedding(VOCAB_SIZE, 100, weights=[embedding_matrix], input_length=MAX_SEQUENCE_LENGTH, trainable=False))\n",
    "    model.add(Masking(mask_value=0.0)) # masking layer, masks any words that don't have an embedding as 0s.\n",
    "    model.add(LSTM(units=32, input_shape=(1, MAX_SEQUENCE_LENGTH)))\n",
    "    model.add(Dense(16))\n",
    "    model.add(Dense(2, activation='softmax'))\n",
    "    \n",
    "    # Compile the model\n",
    "    model.compile(\n",
    "    optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "    # summarize the model\n",
    "    model.summary()\n",
    "    \n",
    "    if plot:\n",
    "        plot_model(model, to_file='model.png', show_shapes=True)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compile Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Helpful Rule of Thumb for Defining # of Parameters in LSTM:\n",
    "\n",
    "$$\n",
    "W = 4dÃ—(n+d)\n",
    "$$\n",
    "Where $d$ is the number of memory cells, and $N$ is the number of dimensions for a data point."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = make_lstm_classification_model()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fit the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fit the model\n",
    "history = model.fit(X_train, y_train,validation_split = 0.1, epochs=5, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "def plot_fit_history(history):\n",
    "    plt.plot(history.history['accuracy'])\n",
    "    plt.plot(history.history['val_accuracy'])\n",
    "    plt.title('model accuracy')\n",
    "    plt.ylabel('accuracy')\n",
    "    plt.xlabel('epoch')\n",
    "    plt.legend(['train', 'val'], loc='upper left')\n",
    "    plt.show()\n",
    "plot_fit_history(history)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluate the model\n",
    "loss, accuracy = model.evaluate(X_test, y_test, verbose=1)\n",
    "print('Accuracy: %f' % (accuracy*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Try Some Random Reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_docs = [\n",
    "    \"Amazing, my son loved it as soon as he opened it.\",\n",
    "    \"Piece of crap junk broke as soon as it was opened.\",\n",
    "    \"Solid toy, it was easy to set up and still works even years later\"\n",
    "]\n",
    "\n",
    "test_docs = list(\n",
    "    map(lambda doc: \" \".join([token.text for token in nlp(doc) if not token.is_stop]), test_docs))\n",
    "\n",
    "encoded_test_sample = integer_encode_documents(test_docs, tokenizer)\n",
    "\n",
    "padded_test_docs = pad_sequences(encoded_test_sample, maxlen=MAX_SEQUENCE_LENGTH, padding='post')\n",
    "\n",
    "prediction = model.predict_classes(padded_test_docs)\n",
    "encoder.inverse_transform(prediction)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem Statement: Predict City of McDonalds Review"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We want to try predict the city that a review corresponds to. For example, given that a McDonalds Yelp review:\n",
    "```\n",
    "\"First they \"\"lost\"\" my order, actually they gave it to someone one else than took 20 minutes to figure out why I was still waiting for my order.They after I was asked what I needed I replied, \"\"my order\"\".They asked for my ticket and the asst mgr looked at the ticket then incompletely filled it.I had to ask her to check to see if she filled it correctly.She acted as if she couldn't be bothered with that so I asked her again.She begrudgingly checked to she did in fact miss something on the ticket.So after 22 minutes I finally had my breakfast biscuit platter.As I left an woman approached and identified herself as the manager, she was dressed as if she had just awoken in an old t-shirt and sweat pants.She said she had heard what happened and said she'd take care of it.Well why didn't she intervene when she saw I was growing annoyed with the incompetence?\"\n",
    "```\n",
    "\n",
    "We want to be able to correctly predict that is is from `Atlanta`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load in McDonalds Yelp Reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reviews_df: pd.DataFrame = pd.read_csv(\n",
    "    \"mcdonalds-yelp-negative-reviews.csv\", encoding=\"latin1\")\n",
    "reviews_df = reviews_df[~reviews_df[\"city\"].isnull()] # filter out null cities\n",
    "\n",
    "reviews_df.sample(len(reviews_df)).head() # shuffle the rows"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get Your Feature Space and Target Labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = reviews_df[\"city\"]\n",
    "docs = reviews_df[\"review\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Perform Label Categorical Encoding of Cities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.utils import to_categorical\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "encoder = LabelEncoder()\n",
    "labels = to_categorical(encoder.fit_transform(labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Remove Stopwords Using SpaCy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "stopwords_removed_docs = list(\n",
    "    map(lambda doc: \" \".join([token.text for token in nlp(doc) if not token.is_stop]), docs))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenize the Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing.text import Tokenizer\n",
    "tokenizer = Tokenizer(num_words=10000, oov_token=\"UNKNOWN_TOKEN\")\n",
    "tokenizer.fit_on_texts(stopwords_removed_docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Integer Encode Tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def integer_encode_documents(docs, tokenizer):\n",
    "    return tokenizer.texts_to_sequences(docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get Max Length Per Token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List\n",
    "def get_max_token_length_per_doc(docs: List[List[str]])-> int:\n",
    "    return max(list(map(lambda x: len(x.split()), docs)))\n",
    "\n",
    "# get the max length in terms of token length\n",
    "max_length = get_max_token_length_per_doc(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing.sequence import pad_sequences\n",
    "MAX_SEQUENCE_LENGTH = 300\n",
    "# integer encode the documents\n",
    "encoded_docs = integer_encode_documents(stopwords_removed_docs, tokenizer)\n",
    "# this is a list of lists, the numbers represent the index position of that word.\n",
    "# for instance, 33 means the 33rd word in the vocabulary\n",
    "padded_docs = pad_sequences(encoded_docs, maxlen=MAX_SEQUENCE_LENGTH, padding='post')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "padded_docs.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split into Train/Test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(padded_docs, labels, test_size=0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Keras RNN Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "VOCAB_SIZE = int(len(tokenizer.word_index) * 1.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load in Glove Vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_glove_vectors():\n",
    "    embeddings_index = {}\n",
    "    with open('glove.6B.100d.txt') as f:\n",
    "        for line in f:\n",
    "            values = line.split()\n",
    "            word = values[0]\n",
    "            coefs = asarray(values[1:], dtype='float32')\n",
    "            embeddings_index[word] = coefs\n",
    "    print('Loaded %s word vectors.' % len(embeddings_index))\n",
    "    return embeddings_index\n",
    "\n",
    "\n",
    "embeddings_index = load_glove_vectors()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load in the Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a weight matrix for words in training docs\n",
    "embedding_matrix = zeros((VOCAB_SIZE, 100))\n",
    "for word, i in tokenizer.word_index.items():\n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    if embedding_vector is not None: # check that it is an actual word that we have embeddings for\n",
    "        embedding_matrix[i] = embedding_vector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers.recurrent import SimpleRNN\n",
    "from keras.layers import Flatten, Masking\n",
    "# define model\n",
    "model = Sequential()\n",
    "model.add(Embedding(VOCAB_SIZE, 100, weights=[embedding_matrix], input_length=MAX_SEQUENCE_LENGTH, trainable=False))\n",
    "model.add(Masking(mask_value=0.0)) # masking layer, masks any words that don't have an embedding as 0s.\n",
    "model.add(SimpleRNN(units=64, input_shape=(1, MAX_SEQUENCE_LENGTH)))\n",
    "model.add(Dense(32))\n",
    "model.add(Dense(9, activation='softmax'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compile the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.utils.vis_utils import plot_model\n",
    "\n",
    "# Compile the model\n",
    "model.compile(\n",
    "    optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "# summarize the model\n",
    "print(model.summary())\n",
    "plot_model(model, to_file='model.png', show_shapes=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fit the Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we start training, let's take a look at the relative distribution of counts for cities. Baseline performance accuracy for this model should be the largest value **~28.4%**. You can get this accuracy by predicting the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reviews_df[\"city\"].value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fit the model\n",
    "model.fit(X_train, y_train, epochs=20, verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluate the model\n",
    "loss, accuracy = model.evaluate(X_test, y_test, verbose=1)\n",
    "print('Accuracy: %f' % (accuracy*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_docs = [\n",
    "    \"Employees look like they hate their job. Milkshake was like drinking milk. Food was cold and not warm at all\",\n",
    "    \"This Mcdonalds is not only in the business of making crappy food and providing even crappier service watch out for the racket they have in the parking lot . If your not careful reading the sign at the the front of the entrance it is going to cost you $195.00 in parking fees. went in to to ask the management they just blew me off. lucky they are in vegas where they dont count on repeat businesssss.\",\n",
    "    \"There are better stores without fruit flies in Griffin, GA.\",\n",
    "    \"Slowest drive-thru ever. Better option is to go to the location on arlington\"\n",
    "]\n",
    "\n",
    "test_docs = list(\n",
    "    map(lambda doc: \" \".join([token.text for token in nlp(doc) if not token.is_stop]), test_docs))\n",
    "\n",
    "encoded_test_sample = integer_encode_documents(test_docs, tokenizer)\n",
    "\n",
    "padded_test_docs = pad_sequences(encoded_test_sample, maxlen=MAX_SEQUENCE_LENGTH, padding='post')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.predict_classes(padded_test_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction = model.predict_classes(padded_test_docs)\n",
    "encoder.inverse_transform(prediction)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "165px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
